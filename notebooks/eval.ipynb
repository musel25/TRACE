{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b8173e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, random\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "from openai import OpenAI\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models as qmodels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afbc4cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is set: True\n",
      "Length: 164\n",
      "Prefix: sk-proj...\n"
     ]
    }
   ],
   "source": [
    "key = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(\"OPENAI_API_KEY is set:\", bool(key))\n",
    "print(\"Length:\", len(key) if key else None)\n",
    "print(\"Prefix:\", (key[:7] + \"...\") if key else None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5c392b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.genai import make_judge\n",
    "\n",
    "JUDGE_NAME = \"xr_config_quality_score\"\n",
    "\n",
    "gt_judge = make_judge(\n",
    "    name=JUDGE_NAME,\n",
    "    instructions=(\n",
    "        \"You are grading an IOS XR telemetry configuration.\\n\\n\"\n",
    "        \"User request (inputs): {{ inputs }}\\n\\n\"\n",
    "        \"Candidate config (outputs): {{ outputs }}\\n\\n\"\n",
    "        \"Reference acceptable config (expectations): {{ expectations }}\\n\\n\"\n",
    "        \"Score quality from 0.0 to 1.0.\\n\"\n",
    "        \"Hard requirements (must match): IP, port, transport (grpc no-tls), encoding.\\n\"\n",
    "        \"Be lenient about names, ordering, and sample-interval unless requested.\\n\"\n",
    "        \"Penalize only if sensor-paths are clearly unrelated.\\n\"\n",
    "        \"Do NOT output telemetry config. Do NOT output code. Keep any explanation extremely short.\"\n",
    "    ),\n",
    "    feedback_value_type=float,\n",
    "    model=\"openai:/gpt-4.1-mini\",\n",
    "    inference_params={\"temperature\": 0, \"max_tokens\": 300},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84bc4d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_cfgs = 16\n",
      "example cfg = RunCfg(vector_db='fixed_window_embeddings', top_k=5, filter_fields={}, temperature=0.0, model_chat='gpt-4.1-nano', model_embed='text-embedding-3-small')\n"
     ]
    }
   ],
   "source": [
    "@dataclass(frozen=True)\n",
    "class RunCfg:\n",
    "    vector_db: str\n",
    "    top_k: int\n",
    "    filter_fields: Dict[str, Any]\n",
    "    temperature: float\n",
    "    model_chat: str\n",
    "    model_embed: str\n",
    "\n",
    "def make_grid(\n",
    "    vector_dbs: List[str],\n",
    "    top_ks: List[int],\n",
    "    filters: List[Dict[str, Any]],\n",
    "    temps: List[float],\n",
    "    chat_models: List[str],\n",
    "    embed_models: List[str],\n",
    ") -> List[RunCfg]:\n",
    "    return [\n",
    "        RunCfg(vdb, k, ff, t, cm, em)\n",
    "        for vdb in vector_dbs\n",
    "        for k in top_ks\n",
    "        for ff in filters\n",
    "        for t in temps\n",
    "        for cm in chat_models\n",
    "        for em in embed_models\n",
    "    ]\n",
    "\n",
    "VECTOR_DBS  = [\"fixed_window_embeddings\", \"catalog_embeddings_improved\"]\n",
    "TOP_KS      = [5, 10]\n",
    "FILTERS     = [{}]                 # e.g. [{\"protocol_tag\": \"bgp\"}]\n",
    "TEMPS       = [0.0, 0.1]\n",
    "CHAT_MODELS = [\"gpt-4.1-nano\", \"gpt-4.1-mini\"]\n",
    "EMBED_MODELS= [\"text-embedding-3-small\"]\n",
    "\n",
    "cfgs = make_grid(VECTOR_DBS, TOP_KS, FILTERS, TEMPS, CHAT_MODELS, EMBED_MODELS)\n",
    "print(\"n_cfgs =\", len(cfgs))\n",
    "print(\"example cfg =\", cfgs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c745cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported tracerag modules OK.\n"
     ]
    }
   ],
   "source": [
    "ROOT = Path.cwd().resolve().parents[0]  # if cwd == repo/notebooks\n",
    "sys.path.insert(0, str(ROOT / \"src\"))\n",
    "\n",
    "from tracerag.rag.naive import naive_rag, build_openai_chat_fn\n",
    "from tracerag.retrieval.qdrant import (\n",
    "    QdrantRetrievalConfig,\n",
    "    build_openai_embedding_fn,\n",
    "    build_qdrant_retriever,\n",
    ")\n",
    "\n",
    "print(\"Imported tracerag modules OK.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "123b8987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System prompt length: 1573\n",
      "Loaded dataset rows: 100\n",
      "Dataset keys: dict_keys(['prompt', 'completion'])\n"
     ]
    }
   ],
   "source": [
    "def load_text(path: Path) -> str:\n",
    "    return path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "def load_jsonl(path: Path) -> List[Dict[str, Any]]:\n",
    "    rows = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "SYSTEM_PROMPT_PATH = Path(\"../data/iosxr_prompt.txt\")\n",
    "DATASET_PATH = Path(\"../data/judge_dataset.jsonl\")\n",
    "\n",
    "SYSTEM_PROMPT = load_text(SYSTEM_PROMPT_PATH)\n",
    "dataset = load_jsonl(DATASET_PATH)\n",
    "\n",
    "print(\"System prompt length:\", len(SYSTEM_PROMPT))\n",
    "print(\"Loaded dataset rows:\", len(dataset))\n",
    "print(\"Dataset keys:\", dataset[0].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecee216f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_qdrant_filter(filter_fields: Dict[str, Any]) -> Optional[qmodels.Filter]:\n",
    "    if not filter_fields:\n",
    "        return None\n",
    "    must = [\n",
    "        qmodels.FieldCondition(key=k, match=qmodels.MatchValue(value=v))\n",
    "        for k, v in filter_fields.items()\n",
    "    ]\n",
    "    return qmodels.Filter(must=must)\n",
    "\n",
    "def make_retriever(cfg: RunCfg, qdrant: QdrantClient, openai_client: OpenAI):\n",
    "    embed_fn = build_openai_embedding_fn(openai_client, model=cfg.model_embed)\n",
    "    q_filter = build_qdrant_filter(cfg.filter_fields)\n",
    "\n",
    "    config = QdrantRetrievalConfig(\n",
    "        collection_name=cfg.vector_db,\n",
    "        top_k=cfg.top_k,\n",
    "        query_filter=q_filter,\n",
    "    )\n",
    "\n",
    "    qdrant_retriever = build_qdrant_retriever(\n",
    "        qdrant=qdrant,\n",
    "        embedding_fn=embed_fn,\n",
    "        config=config,\n",
    "    )\n",
    "    return lambda query, k: qdrant_retriever(query, top_k=k, query_filter=q_filter)\n",
    "\n",
    "def make_chat_fn(cfg: RunCfg, openai_client: OpenAI):\n",
    "    return build_openai_chat_fn(\n",
    "        openai_client=openai_client,\n",
    "        model=cfg.model_chat,\n",
    "        temperature=cfg.temperature,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adcf1e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_predict_one(prompt: str, cfg: RunCfg, qdrant: QdrantClient, openai_client: OpenAI) -> str:\n",
    "    retriever = make_retriever(cfg, qdrant, openai_client)\n",
    "    chat_fn = make_chat_fn(cfg, openai_client)\n",
    "\n",
    "    resp = naive_rag(\n",
    "        user_query=prompt,\n",
    "        retriever=retriever,\n",
    "        chat_fn=chat_fn,\n",
    "        system_prompt=SYSTEM_PROMPT,\n",
    "        top_k=cfg.top_k,\n",
    "        answer_instruction=\"Return only IOS XR telemetry configuration.\",\n",
    "    )\n",
    "    return resp.answer\n",
    "\n",
    "def build_eval_data(\n",
    "    dataset_rows: List[Dict[str, Any]],\n",
    "    cfg: RunCfg,\n",
    "    qdrant: QdrantClient,\n",
    "    openai_client: OpenAI,\n",
    "    max_examples: Optional[int] = None,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    n = len(dataset_rows) if max_examples is None else min(len(dataset_rows), max_examples)\n",
    "    out = []\n",
    "    for i in range(n):\n",
    "        prompt = dataset_rows[i][\"prompt\"]\n",
    "        reference = dataset_rows[i][\"completion\"]\n",
    "        candidate = rag_predict_one(prompt, cfg, qdrant, openai_client)\n",
    "        out.append({\n",
    "            \"inputs\": {\"prompt\": prompt},\n",
    "            \"outputs\": {\"response\": candidate},\n",
    "            \"expectations\": {\"expected_response\": reference},\n",
    "        })\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b29c566b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _safe_json(x):\n",
    "    if x is None:\n",
    "        return None\n",
    "    if isinstance(x, (dict, list)):\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip()\n",
    "        if not s:\n",
    "            return None\n",
    "        try:\n",
    "            return json.loads(s)\n",
    "        except Exception:\n",
    "            return {\"_raw\": s}\n",
    "    return {\"_raw\": str(x)}\n",
    "\n",
    "def export_run_traces_full(run_id: str, judge_name: str = JUDGE_NAME) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Per-example table for ONE mlflow run_id:\n",
    "      prompt, candidate, expected, score, rationale, trace_id + run params.\n",
    "    \"\"\"\n",
    "    client = MlflowClient()\n",
    "    run = client.get_run(run_id)\n",
    "    run_params = dict(run.data.params)\n",
    "\n",
    "    # Find trace ids for this run (fast)\n",
    "    traces_df = mlflow.search_traces(filter_string=f\"trace.run_id = '{run_id}'\")\n",
    "    trace_ids = list(traces_df[\"trace_id\"].values) if \"trace_id\" in traces_df.columns else []\n",
    "\n",
    "    rows = []\n",
    "    for trace_id in trace_ids:\n",
    "        t = mlflow.get_trace(trace_id)\n",
    "\n",
    "        req = _safe_json(getattr(t.data, \"request\", None))\n",
    "        resp = _safe_json(getattr(t.data, \"response\", None))\n",
    "\n",
    "        prompt = None\n",
    "        expected = None\n",
    "        candidate = None\n",
    "\n",
    "        if isinstance(req, dict):\n",
    "            inputs = req.get(\"inputs\") or {}\n",
    "            expectations = req.get(\"expectations\") or {}\n",
    "            prompt = inputs.get(\"prompt\") or req.get(\"prompt\")\n",
    "            expected = expectations.get(\"expected_response\") or expectations.get(\"reference\")\n",
    "\n",
    "        if isinstance(resp, dict):\n",
    "            outputs = resp.get(\"outputs\") or {}\n",
    "            candidate = outputs.get(\"response\") or resp.get(\"response\")\n",
    "\n",
    "        score = None\n",
    "        rationale = None\n",
    "        assessments = getattr(t.info, \"assessments\", None) or []\n",
    "        for a in assessments:\n",
    "            if getattr(a, \"assessment_name\", None) == judge_name:\n",
    "                fb = getattr(a, \"feedback\", None)\n",
    "                score = getattr(fb, \"value\", None) if fb is not None else None\n",
    "                rationale = getattr(a, \"rationale\", None)\n",
    "\n",
    "        rows.append({\n",
    "            \"run_id\": run_id,\n",
    "            \"trace_id\": trace_id,\n",
    "            \"prompt\": prompt,\n",
    "            \"candidate\": candidate,\n",
    "            \"expected\": expected,\n",
    "            \"score\": score,\n",
    "            \"rationale\": rationale,\n",
    "            **{f\"param.{k}\": v for k, v in run_params.items()},\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    if \"score\" in df.columns:\n",
    "        df[\"score\"] = pd.to_numeric(df[\"score\"], errors=\"coerce\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7762dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_cfg_params(cfg: RunCfg):\n",
    "    mlflow.log_params({\n",
    "        \"vector_db\": cfg.vector_db,\n",
    "        \"top_k\": cfg.top_k,\n",
    "        \"filter_fields\": json.dumps(cfg.filter_fields, sort_keys=True),\n",
    "        \"temperature\": cfg.temperature,\n",
    "        \"model_chat\": cfg.model_chat,\n",
    "        \"model_embed\": cfg.model_embed,\n",
    "    })\n",
    "\n",
    "def log_eval_aggregates(df: pd.DataFrame):\n",
    "    mlflow.log_metric(\"judge_mean\", float(df[\"score\"].mean()))\n",
    "    mlflow.log_metric(\"judge_min\", float(df[\"score\"].min()))\n",
    "    mlflow.log_metric(\"judge_pass_rate_ge_0.8\", float((df[\"score\"] >= 0.8).mean()))\n",
    "    mlflow.log_metric(\"n_examples\", int(df[\"score\"].notna().sum()))\n",
    "\n",
    "def run_one_cfg_mlflow(\n",
    "    cfg: RunCfg,\n",
    "    dataset_rows: List[Dict[str, Any]],\n",
    "    qdrant: QdrantClient,\n",
    "    openai_client: OpenAI,\n",
    "    max_examples: Optional[int] = None,\n",
    "):\n",
    "    log_cfg_params(cfg)\n",
    "\n",
    "    eval_data = build_eval_data(dataset_rows, cfg, qdrant, openai_client, max_examples=max_examples)\n",
    "\n",
    "    # This produces traces + assessments\n",
    "    _ = mlflow.genai.evaluate(data=eval_data, scorers=[gt_judge])\n",
    "\n",
    "    run_id = mlflow.active_run().info.run_id\n",
    "\n",
    "    # Extract full per-example evidence\n",
    "    df = export_run_traces_full(run_id, judge_name=JUDGE_NAME)\n",
    "\n",
    "    # Aggregate metrics in the run\n",
    "    log_eval_aggregates(df)\n",
    "\n",
    "    # Persist per-example table\n",
    "    out_csv = \"per_example_eval.csv\"\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    mlflow.log_artifact(out_csv)\n",
    "\n",
    "    # Nice UI table (if supported by your MLflow version)\n",
    "    # mlflow.log_table(df, \"per_example_eval.json\")\n",
    "\n",
    "    return df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
