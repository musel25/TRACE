{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d388ba40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/musel/Documents/github/TRACE/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: /home/musel/Documents/github/TRACE/notebooks\n",
      "src exists: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167202/2938487129.py:15: TRLExperimentalWarning: You are importing from 'trl.experimental'. APIs here are unstable and may change or be removed without notice. Silence this warning by setting environment variable TRL_EXPERIMENTAL_SILENCE=1.\n",
      "  from trl.experimental.judges import HfPairwiseJudge\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Sequence, Tuple\n",
    "\n",
    "import mlflow\n",
    "\n",
    "from openai import OpenAI\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models as qmodels\n",
    "\n",
    "# TRL judge\n",
    "from trl.experimental.judges import HfPairwiseJudge\n",
    "\n",
    "# ---- Make sure we can import your local project modules ----\n",
    "ROOT = Path.cwd()\n",
    "# if your notebook is in repo root, ROOT is correct; otherwise set ROOT manually\n",
    "sys.path.insert(0, str(ROOT / \"src\"))\n",
    "\n",
    "print(\"ROOT:\", ROOT)\n",
    "print(\"src exists:\", (ROOT / \"src\").exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2392df4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd().resolve().parents[0]  # repo root if cwd==repo/notebooks\n",
    "sys.path.insert(0, str(ROOT / \"src\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22df5387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported tracerag modules OK.\n"
     ]
    }
   ],
   "source": [
    "from tracerag.rag.naive import naive_rag, build_openai_chat_fn\n",
    "from tracerag.retrieval.qdrant import (\n",
    "    QdrantRetrievalConfig,\n",
    "    build_openai_embedding_fn,\n",
    "    build_qdrant_retriever,\n",
    ")\n",
    "\n",
    "print(\"Imported tracerag modules OK.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ec501b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded rows: 2500\n",
      "Example keys: dict_keys(['prompt', 'completion'])\n",
      "Prompt preview:\n",
      " Can you generate telemetry configuration for cisco ios xr about border gateway protocol (BGP)? Use grpc with no tls, the telemetry server address is 192.0.2.100 with port 57500. Choose relevant sensor\n",
      "Completion preview:\n",
      " telemetry model-driven\n",
      " sensor-group BGP-STATS\n",
      "  sensor-path Cisco-IOS-XR-ipv4-bgp-oper:bgp/instances/instance/instance-active/default-vrf/sessions\n",
      "  sensor-path Cisco-IOS-XR-ipv4-bgp-oper:bgp/instanc\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = Path(\"../data/judge_dataset.jsonl\")  # <-- change if needed\n",
    "\n",
    "def load_jsonl(path: Path) -> List[Dict[str, Any]]:\n",
    "    rows = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "dataset = load_jsonl(DATASET_PATH)\n",
    "print(\"Loaded rows:\", len(dataset))\n",
    "print(\"Example keys:\", dataset[0].keys())\n",
    "print(\"Prompt preview:\\n\", dataset[0][\"prompt\"][:200])\n",
    "print(\"Completion preview:\\n\", dataset[0][\"completion\"][:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1280f8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "dataset = random.sample(dataset, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2eba9303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System prompt length: 1573\n",
      "You are a Cisco IOS XR network engineer generating IOS XR 7.x model-driven telemetry configuration.\n",
      "\n",
      "INPUTS:\n",
      "- USER_REQUEST: describes the intent (e.g., “BGP telemetry”), destination IP/port, and optionally interval.\n",
      "- CONTEXT: a list of valid YANG sensor-path candidates.\n",
      "\n",
      "HARD RULES:\n",
      "- Output ONLY \n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT_PATH = Path(\"../data/iosxr_prompt.txt\")  # <-- your file\n",
    "SYSTEM_PROMPT = SYSTEM_PROMPT_PATH.read_text(encoding=\"utf-8\")\n",
    "\n",
    "print(\"System prompt length:\", len(SYSTEM_PROMPT))\n",
    "print(SYSTEM_PROMPT[:300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff669a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class RunCfg:\n",
    "    vector_db: str                 # we'll map this to a Qdrant collection (or backend)\n",
    "    top_k: int\n",
    "    filter_fields: Dict[str, Any]  # e.g., {\"domain\":\"bgp\"} or {}\n",
    "    temperature: float\n",
    "    model_chat: str\n",
    "    model_embed: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56ffe994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_qdrant_filter(filter_fields: Dict[str, Any]) -> Optional[qmodels.Filter]:\n",
    "    if not filter_fields:\n",
    "        return None\n",
    "\n",
    "    must: List[qmodels.FieldCondition] = []\n",
    "    for key, value in filter_fields.items():\n",
    "        must.append(\n",
    "            qmodels.FieldCondition(\n",
    "                key=key,\n",
    "                match=qmodels.MatchValue(value=value),\n",
    "            )\n",
    "        )\n",
    "    return qmodels.Filter(must=must)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba3ec92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_retriever(\n",
    "    *,\n",
    "    cfg: RunCfg,\n",
    "    qdrant: QdrantClient,\n",
    "    openai_client: OpenAI,\n",
    "):\n",
    "    embed_fn = build_openai_embedding_fn(openai_client, model=cfg.model_embed)\n",
    "\n",
    "    q_filter = build_qdrant_filter(cfg.filter_fields)\n",
    "\n",
    "    config = QdrantRetrievalConfig(\n",
    "        collection_name=cfg.vector_db,  # <-- mapping: vector_db -> collection\n",
    "        top_k=cfg.top_k,\n",
    "        query_filter=q_filter,\n",
    "    )\n",
    "\n",
    "    qdrant_retriever = build_qdrant_retriever(\n",
    "        qdrant=qdrant,\n",
    "        embedding_fn=embed_fn,\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    # Return a function: retriever(query, k) -> List[Chunk]\n",
    "    # Keep filter fixed per cfg (since filter_fields is a variation)\n",
    "    return lambda query, k: qdrant_retriever(query, top_k=k, query_filter=q_filter)\n",
    "\n",
    "\n",
    "def make_chat_fn(cfg: RunCfg, openai_client: OpenAI):\n",
    "    return build_openai_chat_fn(\n",
    "        openai_client,\n",
    "        model=cfg.model_chat,\n",
    "        temperature=cfg.temperature,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a10b6fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANSWER_INSTRUCTION = \"Return only IOS XR telemetry configuration.\"\n",
    "\n",
    "def generate_completion(\n",
    "    *,\n",
    "    prompt: str,\n",
    "    cfg: RunCfg,\n",
    "    qdrant: QdrantClient,\n",
    "    openai_client: OpenAI,\n",
    ") -> Dict[str, Any]:\n",
    "    retriever = make_retriever(cfg=cfg, qdrant=qdrant, openai_client=openai_client)\n",
    "    chat_fn = make_chat_fn(cfg, openai_client=openai_client)\n",
    "\n",
    "    resp = naive_rag(\n",
    "        user_query=prompt,\n",
    "        retriever=retriever,\n",
    "        chat_fn=chat_fn,\n",
    "        system_prompt=SYSTEM_PROMPT,\n",
    "        top_k=cfg.top_k,\n",
    "        answer_instruction=ANSWER_INSTRUCTION,\n",
    "    )\n",
    "\n",
    "    # Return full trace (useful for MLflow artifacts)\n",
    "    return {\n",
    "        \"answer\": resp.answer,\n",
    "        \"context_preview\": (resp.context or \"\")[:1200],\n",
    "        \"retrieved\": [\n",
    "            {\n",
    "                \"id\": ch.id,\n",
    "                \"score\": ch.score,\n",
    "                \"file_path\": ch.file_path,\n",
    "                \"chunk_index\": ch.chunk_index,\n",
    "                # if catalog payload exists, keep these fields\n",
    "                \"module\": (ch.payload or {}).get(\"module\"),\n",
    "                \"path\": (ch.payload or {}).get(\"path\"),\n",
    "                \"domain\": (ch.payload or {}).get(\"domain\") or (ch.payload or {}).get(\"protocol_tag\"),\n",
    "                \"category\": (ch.payload or {}).get(\"category\"),\n",
    "            }\n",
    "            for ch in (resp.chunks or [])\n",
    "        ],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5517d5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = HfPairwiseJudge()\n",
    "\n",
    "def judge_binary_score(prompt: str, candidate: str, reference: str) -> int:\n",
    "    # returns 0 if candidate wins, 1 if reference wins\n",
    "    winner_idx = judge.judge(\n",
    "        prompts=[prompt],\n",
    "        completions=[[candidate, reference]],\n",
    "    )[0]\n",
    "\n",
    "    # Convert to {0,1}: 1 means candidate wins\n",
    "    return 1 if winner_idx == 0 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2263ec50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_cfg_and_log_mlflow(\n",
    "    *,\n",
    "    cfg: RunCfg,\n",
    "    dataset: Sequence[Dict[str, Any]],\n",
    "    qdrant: QdrantClient,\n",
    "    openai_client: OpenAI,\n",
    "    max_examples: Optional[int] = None,\n",
    "):\n",
    "    # --- log only your variations as params ---\n",
    "    mlflow.log_params({\n",
    "        \"vector_db\": cfg.vector_db,\n",
    "        \"top_k\": cfg.top_k,\n",
    "        \"filter_fields\": json.dumps(cfg.filter_fields, sort_keys=True),\n",
    "        \"temperature\": cfg.temperature,\n",
    "        \"model_chat\": cfg.model_chat,\n",
    "        \"model_embed\": cfg.model_embed,\n",
    "    })\n",
    "\n",
    "    rows = []\n",
    "    scores = []\n",
    "\n",
    "    n = len(dataset) if max_examples is None else min(len(dataset), max_examples)\n",
    "\n",
    "    for i in range(n):\n",
    "        prompt = dataset[i][\"prompt\"]\n",
    "        reference = dataset[i][\"completion\"]\n",
    "\n",
    "        gen = generate_completion(\n",
    "            prompt=prompt,\n",
    "            cfg=cfg,\n",
    "            qdrant=qdrant,\n",
    "            openai_client=openai_client,\n",
    "        )\n",
    "\n",
    "        candidate = gen[\"answer\"]\n",
    "        score = judge_binary_score(prompt, candidate, reference)\n",
    "\n",
    "        scores.append(score)\n",
    "        rows.append({\n",
    "            \"i\": i,\n",
    "            \"prompt\": prompt,\n",
    "            \"reference\": reference,\n",
    "            \"candidate\": candidate,\n",
    "            \"judge_score\": score,\n",
    "            \"retrieved\": gen[\"retrieved\"],\n",
    "            \"context_preview\": gen[\"context_preview\"],\n",
    "        })\n",
    "\n",
    "    mean_score = sum(scores) / max(1, len(scores))\n",
    "    mlflow.log_metric(\"judge_mean\", mean_score)\n",
    "    mlflow.log_metric(\"n_examples\", len(scores))\n",
    "\n",
    "    # artifact\n",
    "    out_dir = Path(\"mlflow_artifacts\")\n",
    "    out_dir.mkdir(exist_ok=True)\n",
    "    out_path = out_dir / \"traces.jsonl\"\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    mlflow.log_artifact(str(out_path))\n",
    "\n",
    "    return mean_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "57ed261b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36,\n",
       " RunCfg(vector_db='catalog_embeddings', top_k=5, filter_fields={}, temperature=0.0, model_chat='gpt-4.1-mini', model_embed='text-embedding-3-small'))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_grid(\n",
    "    *,\n",
    "    vector_dbs: Sequence[str],\n",
    "    top_ks: Sequence[int],\n",
    "    filter_fields_list: Sequence[Dict[str, Any]],\n",
    "    temperatures: Sequence[float],\n",
    "    model_chats: Sequence[str],\n",
    "    model_embeds: Sequence[str],\n",
    ") -> List[RunCfg]:\n",
    "    cfgs = []\n",
    "    for vdb in vector_dbs:\n",
    "        for k in top_ks:\n",
    "            for ff in filter_fields_list:\n",
    "                for temp in temperatures:\n",
    "                    for chat in model_chats:\n",
    "                        for emb in model_embeds:\n",
    "                            cfgs.append(RunCfg(\n",
    "                                vector_db=vdb,\n",
    "                                top_k=k,\n",
    "                                filter_fields=ff,\n",
    "                                temperature=temp,\n",
    "                                model_chat=chat,\n",
    "                                model_embed=emb,\n",
    "                            ))\n",
    "    return cfgs\n",
    "\n",
    "\n",
    "# Example: fill these with YOUR options\n",
    "VECTOR_DBS = [\n",
    "    \"catalog_embeddings\",\n",
    "    \"fixed_window_embeddings\",\n",
    "    # \"your_third_collection_or_db\",\n",
    "]\n",
    "\n",
    "TOP_KS = [5, 10, 15]\n",
    "\n",
    "FILTERS = [\n",
    "    {},  # no filter\n",
    "    {\"protocol_tag\": \"bgp\"},  # if that's what you have today\n",
    "    # {\"domain\": \"bgp\"},  # after you add domain to catalog\n",
    "    # {\"module_family\": \"ipv4-bgp-oper\"},  # if you store it\n",
    "]\n",
    "\n",
    "TEMPS = [0.0, 0.1, 0.2]\n",
    "\n",
    "MODEL_CHATS = [\"gpt-4.1-mini\"]\n",
    "MODEL_EMBEDS = [\"text-embedding-3-small\"]\n",
    "\n",
    "cfgs = make_grid(\n",
    "    vector_dbs=VECTOR_DBS,\n",
    "    top_ks=TOP_KS,\n",
    "    filter_fields_list=FILTERS,\n",
    "    temperatures=TEMPS,\n",
    "    model_chats=MODEL_CHATS,\n",
    "    model_embeds=MODEL_EMBEDS,\n",
    ")\n",
    "\n",
    "len(cfgs), cfgs[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "694146e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/musel/Documents/github/TRACE/.venv/lib/python3.11/site-packages/mlflow/tracking/_tracking_service/utils.py:178: FutureWarning: The filesystem tracking backend (e.g., './mlruns') will be deprecated in February 2026. Consider transitioning to a database backend (e.g., 'sqlite:///mlflow.db') to take advantage of the latest MLflow features. See https://github.com/mlflow/mlflow/issues/18534 for more details and migration guidance. For migrating existing data, https://github.com/mlflow/mlflow-export-import can be used.\n",
      "  return FileStore(store_uri, store_uri)\n",
      "2026/01/11 22:32:15 INFO mlflow.tracking.fluent: Experiment with name 'xr_rag_judge_sweep' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "catalog_embeddings|k=5|t=0.0|chat=gpt-4.1-mini|emb=text-embedding-3-small|f={} judge_mean = 0.5\n",
      "catalog_embeddings|k=5|t=0.1|chat=gpt-4.1-mini|emb=text-embedding-3-small|f={} judge_mean = 0.2\n",
      "catalog_embeddings|k=5|t=0.2|chat=gpt-4.1-mini|emb=text-embedding-3-small|f={} judge_mean = 0.4\n",
      "catalog_embeddings|k=5|t=0.0|chat=gpt-4.1-mini|emb=text-embedding-3-small|f={\"protocol_tag\": \"bgp\"} judge_mean = 0.3\n",
      "catalog_embeddings|k=5|t=0.1|chat=gpt-4.1-mini|emb=text-embedding-3-small|f={\"protocol_tag\": \"bgp\"} judge_mean = 0.4\n",
      "catalog_embeddings|k=5|t=0.2|chat=gpt-4.1-mini|emb=text-embedding-3-small|f={\"protocol_tag\": \"bgp\"} judge_mean = 0.2\n"
     ]
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "402 Client Error: Payment Required for url: https://router.huggingface.co/novita/v3/openai/chat/completions (Request ID: Root=1-696417be-6652319b03bb1c4f02ef29bf;c0614f17-8bbd-4803-a67c-782f5ddb5fca)\n\nYou have reached the free monthly usage limit for novita. Subscribe to PRO to get 20x more included usage, or add pre-paid credits to your account.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/TRACE/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:402\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/TRACE/.venv/lib/python3.11/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 402 Client Error: Payment Required for url: https://router.huggingface.co/novita/v3/openai/chat/completions",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mHfHubHTTPError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     13\u001b[39m run_name = (\n\u001b[32m     14\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg.vector_db\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m|k=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg.top_k\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m|t=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg.temperature\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m|\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     15\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mchat=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg.model_chat\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m|emb=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg.model_embed\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m|f=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjson.dumps(cfg.filter_fields,\u001b[38;5;250m \u001b[39msort_keys=\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     16\u001b[39m )\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m mlflow.start_run(run_name=run_name, nested=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     mean_score = \u001b[43meval_cfg_and_log_mlflow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43mqdrant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mqdrant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[43mopenai_client\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopenai_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_examples\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# or set e.g. 50 for faster iteration\u001b[39;49;00m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28mprint\u001b[39m(run_name, \u001b[33m\"\u001b[39m\u001b[33mjudge_mean =\u001b[39m\u001b[33m\"\u001b[39m, mean_score)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36meval_cfg_and_log_mlflow\u001b[39m\u001b[34m(cfg, dataset, qdrant, openai_client, max_examples)\u001b[39m\n\u001b[32m     28\u001b[39m gen = generate_completion(\n\u001b[32m     29\u001b[39m     prompt=prompt,\n\u001b[32m     30\u001b[39m     cfg=cfg,\n\u001b[32m     31\u001b[39m     qdrant=qdrant,\n\u001b[32m     32\u001b[39m     openai_client=openai_client,\n\u001b[32m     33\u001b[39m )\n\u001b[32m     35\u001b[39m candidate = gen[\u001b[33m\"\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m score = \u001b[43mjudge_binary_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreference\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m scores.append(score)\n\u001b[32m     39\u001b[39m rows.append({\n\u001b[32m     40\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mi\u001b[39m\u001b[33m\"\u001b[39m: i,\n\u001b[32m     41\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m: prompt,\n\u001b[32m   (...)\u001b[39m\u001b[32m     46\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcontext_preview\u001b[39m\u001b[33m\"\u001b[39m: gen[\u001b[33m\"\u001b[39m\u001b[33mcontext_preview\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     47\u001b[39m })\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mjudge_binary_score\u001b[39m\u001b[34m(prompt, candidate, reference)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mjudge_binary_score\u001b[39m(prompt: \u001b[38;5;28mstr\u001b[39m, candidate: \u001b[38;5;28mstr\u001b[39m, reference: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m      4\u001b[39m     \u001b[38;5;66;03m# returns 0 if candidate wins, 1 if reference wins\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     winner_idx = \u001b[43mjudge\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjudge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcandidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreference\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# Convert to {0,1}: 1 means candidate wins\u001b[39;00m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m winner_idx == \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/TRACE/.venv/lib/python3.11/site-packages/trl/experimental/judges/judges.py:327\u001b[39m, in \u001b[36mHfPairwiseJudge.judge\u001b[39m\u001b[34m(self, prompts, completions, shuffle_order)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;66;03m# Call the completions concurrently\u001b[39;00m\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m concurrent.futures.ThreadPoolExecutor() \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     ranks = \u001b[38;5;28mlist\u001b[39m(executor.map(get_rank, prompts, completions))\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# Flip back the ranks to the original order if needed\u001b[39;00m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m shuffle_order:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.8-linux-x86_64-gnu/lib/python3.11/concurrent/futures/_base.py:619\u001b[39m, in \u001b[36mExecutor.map.<locals>.result_iterator\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[32m    617\u001b[39m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[32m    618\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    621\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs.pop(), end_time - time.monotonic())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.8-linux-x86_64-gnu/lib/python3.11/concurrent/futures/_base.py:317\u001b[39m, in \u001b[36m_result_or_cancel\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    319\u001b[39m         fut.cancel()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.8-linux-x86_64-gnu/lib/python3.11/concurrent/futures/_base.py:456\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.8-linux-x86_64-gnu/lib/python3.11/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.8-linux-x86_64-gnu/lib/python3.11/concurrent/futures/thread.py:58\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/TRACE/.venv/lib/python3.11/site-packages/trl/experimental/judges/judges.py:317\u001b[39m, in \u001b[36mHfPairwiseJudge.judge.<locals>.get_rank\u001b[39m\u001b[34m(prompt, candidates)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_rank\u001b[39m(prompt, candidates):\n\u001b[32m    316\u001b[39m     content = \u001b[38;5;28mself\u001b[39m.system_prompt.format(prompt=prompt, response0=candidates[\u001b[32m0\u001b[39m], response1=candidates[\u001b[32m1\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m     completion = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m     response = completion.choices[\u001b[32m0\u001b[39m].message.content\n\u001b[32m    319\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33m0\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/TRACE/.venv/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:915\u001b[39m, in \u001b[36mInferenceClient.chat_completion\u001b[39m\u001b[34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p, extra_body)\u001b[39m\n\u001b[32m    887\u001b[39m parameters = {\n\u001b[32m    888\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: payload_model,\n\u001b[32m    889\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   (...)\u001b[39m\u001b[32m    906\u001b[39m     **(extra_body \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[32m    907\u001b[39m }\n\u001b[32m    908\u001b[39m request_parameters = provider_helper.prepare_request(\n\u001b[32m    909\u001b[39m     inputs=messages,\n\u001b[32m    910\u001b[39m     parameters=parameters,\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m     api_key=\u001b[38;5;28mself\u001b[39m.token,\n\u001b[32m    914\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inner_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _stream_chat_completion_response(data)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/TRACE/.venv/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:275\u001b[39m, in \u001b[36mInferenceClient._inner_post\u001b[39m\u001b[34m(self, request_parameters, stream)\u001b[39m\n\u001b[32m    272\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_parameters.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.iter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response.content\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/TRACE/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:475\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    471\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    473\u001b[39m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[32m    474\u001b[39m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mHfHubHTTPError\u001b[39m: 402 Client Error: Payment Required for url: https://router.huggingface.co/novita/v3/openai/chat/completions (Request ID: Root=1-696417be-6652319b03bb1c4f02ef29bf;c0614f17-8bbd-4803-a67c-782f5ddb5fca)\n\nYou have reached the free monthly usage limit for novita. Subscribe to PRO to get 20x more included usage, or add pre-paid credits to your account."
     ]
    }
   ],
   "source": [
    "# Configure MLflow location (local)\n",
    "mlflow.set_tracking_uri(\"file://\" + str((Path.cwd() / \"../mlruns\").resolve()))\n",
    "mlflow.set_experiment(\"xr_rag_judge_sweep\")\n",
    "\n",
    "# Clients\n",
    "qdrant = QdrantClient(host=\"localhost\", port=6333)  # adjust if cloud\n",
    "openai_client = OpenAI()\n",
    "\n",
    "with mlflow.start_run(run_name=\"parent_sweep\") as parent:\n",
    "    mlflow.set_tag(\"type\", \"parent\")\n",
    "\n",
    "    for cfg in cfgs:\n",
    "        run_name = (\n",
    "            f\"{cfg.vector_db}|k={cfg.top_k}|t={cfg.temperature}|\"\n",
    "            f\"chat={cfg.model_chat}|emb={cfg.model_embed}|f={json.dumps(cfg.filter_fields, sort_keys=True)}\"\n",
    "        )\n",
    "\n",
    "        with mlflow.start_run(run_name=run_name, nested=True):\n",
    "            mean_score = eval_cfg_and_log_mlflow(\n",
    "                cfg=cfg,\n",
    "                dataset=dataset,\n",
    "                qdrant=qdrant,\n",
    "                openai_client=openai_client,\n",
    "                max_examples=None,  # or set e.g. 50 for faster iteration\n",
    "            )\n",
    "            print(run_name, \"judge_mean =\", mean_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a05f93",
   "metadata": {},
   "source": [
    "mlflow ui --backend-store-uri file:./mlruns\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
