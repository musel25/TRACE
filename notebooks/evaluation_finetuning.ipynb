{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9aa5141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is set: True\n",
      "Length: 164\n",
      "Prefix: sk-proj...\n",
      "Tracking: file:///home/musel/Documents/github/TRACE/notebooks/mlruns\n",
      "Experiment: xr_hf_llm_judge_sweep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/musel/Documents/github/TRACE/.venv/lib/python3.11/site-packages/mlflow/tracking/_tracking_service/utils.py:178: FutureWarning: The filesystem tracking backend (e.g., './mlruns') will be deprecated in February 2026. Consider transitioning to a database backend (e.g., 'sqlite:///mlflow.db') to take advantage of the latest MLflow features. See https://github.com/mlflow/mlflow/issues/18534 for more details and migration guidance. For migrating existing data, https://github.com/mlflow/mlflow-export-import can be used.\n",
      "  return FileStore(store_uri, store_uri)\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 — MLflow tracking + judge (same idea as your RAG code)\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import mlflow\n",
    "from mlflow.genai import make_judge\n",
    "\n",
    "key = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(\"OPENAI_API_KEY is set:\", bool(key))\n",
    "print(\"Length:\", len(key) if key else None)\n",
    "print(\"Prefix:\", (key[:7] + \"...\") if key else None)\n",
    "\n",
    "TRACKING_DIR = (Path.cwd() / \"mlruns\").resolve()\n",
    "mlflow.set_tracking_uri(\"file://\" + str(TRACKING_DIR))\n",
    "\n",
    "EXPERIMENT_NAME = \"xr_hf_llm_judge_sweep\"\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "print(\"Tracking:\", mlflow.get_tracking_uri())\n",
    "print(\"Experiment:\", EXPERIMENT_NAME)\n",
    "\n",
    "JUDGE_NAME = \"xr_config_quality_score\"\n",
    "\n",
    "gt_judge = make_judge(\n",
    "    name=JUDGE_NAME,\n",
    "    instructions=(\n",
    "        \"You are grading an IOS XR telemetry configuration.\\n\\n\"\n",
    "        \"User request (inputs): {{ inputs }}\\n\\n\"\n",
    "        \"Candidate config (outputs): {{ outputs }}\\n\\n\"\n",
    "        \"Reference acceptable config (expectations): {{ expectations }}\\n\\n\"\n",
    "        \"Score quality from 0.0 to 1.0.\\n\"\n",
    "        \"Hard requirements (must match): IP, port, transport (grpc no-tls), encoding.\\n\"\n",
    "        \"Be lenient about names, ordering, and sample-interval unless requested.\\n\"\n",
    "        \"Penalize only if sensor-paths are clearly unrelated.\\n\"\n",
    "        \"Do NOT output telemetry config. Do NOT output code. Keep any explanation extremely short.\"\n",
    "    ),\n",
    "    feedback_value_type=float,\n",
    "    model=\"openai:/gpt-4.1-mini\",\n",
    "    inference_params={\"temperature\": 0, \"max_tokens\": 300},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a812deaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 100 rows\n",
      "Keys: dict_keys(['prompt', 'completion'])\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 — load your dataset JSONL (prompt + completion)\n",
    "\n",
    "import json\n",
    "from typing import Any, Dict, List\n",
    "from pathlib import Path\n",
    "\n",
    "def load_jsonl(path: Path) -> List[Dict[str, Any]]:\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            s = line.strip()\n",
    "            if not s:\n",
    "                continue\n",
    "            rows.append(json.loads(s))\n",
    "    return rows\n",
    "\n",
    "DATASET_PATH = Path(\"../data/judge_dataset.jsonl\")  # adjust\n",
    "dataset_rows = load_jsonl(DATASET_PATH)\n",
    "print(\"Loaded:\", len(dataset_rows), \"rows\")\n",
    "print(\"Keys:\", dataset_rows[0].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f40fc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 — system prompt variants (sweep these)\n",
    "\n",
    "BASE_SYSTEM_PROMPT = \"\"\n",
    "PROMPT_ENGINEERING_PATH = Path(\"../data/iosxr_prompt.txt\")  # adjust\n",
    "PROMPT_ENGINEERING = PROMPT_ENGINEERING_PATH.read_text(encoding=\"utf-8\") if PROMPT_ENGINEERING_PATH.exists() else \"\"\n",
    "\n",
    "PROMPT_VARIANTS = [\n",
    "    (\"base\", BASE_SYSTEM_PROMPT),\n",
    "    (\"strict\", (BASE_SYSTEM_PROMPT + \"\\n\" + PROMPT_ENGINEERING).strip()),\n",
    "]\n",
    "\n",
    "ANSWER_INSTRUCTION = \"Return only IOS XR telemetry configuration. No markdown. No explanation.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b1d785a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/musel/Documents/github/TRACE/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 (REPLACE) — PEFT adapter loading + generation (with caching)\n",
    "\n",
    "import json\n",
    "from typing import Any, Dict, Tuple\n",
    "import torch\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# cache: adapter_id -> (tokenizer, model, base_id)\n",
    "_MODEL_CACHE: Dict[str, Tuple[Any, Any, str]] = {}\n",
    "\n",
    "def read_adapter_config(adapter_id: str) -> Dict[str, Any]:\n",
    "    p = hf_hub_download(repo_id=adapter_id, filename=\"adapter_config.json\")\n",
    "    with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_hf_adapter_model(\n",
    "    adapter_id: str,\n",
    "    *,\n",
    "    use_4bit: bool = False,\n",
    ") -> Tuple[Any, Any, str]:\n",
    "    \"\"\"\n",
    "    Loads a PEFT adapter repo by:\n",
    "      1) reading adapter_config.json for base_model_name_or_path\n",
    "      2) loading the base model\n",
    "      3) attaching the adapter via PeftModel.from_pretrained\n",
    "    \"\"\"\n",
    "    if adapter_id in _MODEL_CACHE:\n",
    "        return _MODEL_CACHE[adapter_id]\n",
    "\n",
    "    acfg = read_adapter_config(adapter_id)\n",
    "    base_id = acfg[\"base_model_name_or_path\"]\n",
    "\n",
    "    # Prefer adapter tokenizer (often includes custom special tokens / chat template files).\n",
    "    tok = AutoTokenizer.from_pretrained(adapter_id, use_fast=True)\n",
    "\n",
    "    # If adapter tokenizer lacks chat template but base has it, borrow it.\n",
    "    if not getattr(tok, \"chat_template\", None):\n",
    "        base_tok = AutoTokenizer.from_pretrained(base_id, use_fast=True)\n",
    "        if getattr(base_tok, \"chat_template\", None):\n",
    "            tok.chat_template = base_tok.chat_template\n",
    "\n",
    "    base_kwargs = dict(\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=\"auto\",\n",
    "    )\n",
    "    if use_4bit:\n",
    "        # Requires bitsandbytes\n",
    "        base_kwargs[\"load_in_4bit\"] = True\n",
    "\n",
    "    base = AutoModelForCausalLM.from_pretrained(base_id, **base_kwargs)\n",
    "    model = PeftModel.from_pretrained(base, adapter_id)  # attach LoRA\n",
    "    model.eval()\n",
    "\n",
    "    # Safety: ensure pad token exists\n",
    "    if tok.pad_token_id is None and tok.eos_token_id is not None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "\n",
    "    _MODEL_CACHE[adapter_id] = (tok, model, base_id)\n",
    "    return tok, model, base_id\n",
    "\n",
    "\n",
    "def format_prompt(tokenizer, system_prompt: str, user_prompt: str) -> str:\n",
    "    sys_txt = (system_prompt or \"\").strip()\n",
    "    usr_txt = user_prompt.strip()\n",
    "\n",
    "    if hasattr(tokenizer, \"apply_chat_template\") and getattr(tokenizer, \"chat_template\", None):\n",
    "        msgs = []\n",
    "        if sys_txt:\n",
    "            msgs.append({\"role\": \"system\", \"content\": sys_txt})\n",
    "        msgs.append({\"role\": \"user\", \"content\": usr_txt})\n",
    "        return tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    # Fallback\n",
    "    if sys_txt:\n",
    "        return f\"SYSTEM:\\n{sys_txt}\\n\\nUSER:\\n{usr_txt}\\n\\nASSISTANT:\\n\"\n",
    "    return f\"USER:\\n{usr_txt}\\n\\nASSISTANT:\\n\"\n",
    "\n",
    "\n",
    "def generation_kwargs_from_temp(temp: float) -> Dict[str, Any]:\n",
    "    if temp <= 0.0:\n",
    "        return dict(do_sample=False)\n",
    "    return dict(do_sample=True, temperature=float(temp), top_p=0.95)\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def hf_predict_one(\n",
    "    *,\n",
    "    model_id: str,            # <- THIS IS YOUR ADAPTER ID (pesimachete/...)\n",
    "    prompt: str,\n",
    "    system_prompt: str,\n",
    "    temperature: float,\n",
    "    max_new_tokens: int = 512,\n",
    "    use_4bit: bool = False,   # <- applies to BASE model load\n",
    ") -> str:\n",
    "    tok, model, base_id = load_hf_adapter_model(model_id, use_4bit=use_4bit)\n",
    "\n",
    "    full_user = (prompt.strip() + \"\\n\\n\" + ANSWER_INSTRUCTION).strip()\n",
    "    text = format_prompt(tok, system_prompt=system_prompt, user_prompt=full_user)\n",
    "\n",
    "    inputs = tok(text, return_tensors=\"pt\")\n",
    "    dev = next(model.parameters()).device\n",
    "    inputs = {k: v.to(dev) for k, v in inputs.items()}\n",
    "\n",
    "    gen_kwargs = dict(\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        eos_token_id=tok.eos_token_id,\n",
    "        pad_token_id=tok.eos_token_id,\n",
    "        **generation_kwargs_from_temp(temperature),\n",
    "    )\n",
    "\n",
    "    out = model.generate(**inputs, **gen_kwargs)\n",
    "    decoded = tok.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    # strip prompt echo if present\n",
    "    if decoded.startswith(text):\n",
    "        decoded = decoded[len(text):]\n",
    "\n",
    "    return decoded.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a6eb956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 — Hugging Face model loading + generation (with caching)\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, Any, Dict\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Cache to avoid re-loading per run\n",
    "_MODEL_CACHE: Dict[str, Tuple[Any, Any]] = {}\n",
    "\n",
    "def load_hf_model(\n",
    "    model_id: str,\n",
    "    *,\n",
    "    use_4bit: bool = False,\n",
    ") -> Tuple[Any, Any]:\n",
    "    \"\"\"\n",
    "    Loads (tokenizer, model). Uses a global cache so you can sweep quickly.\n",
    "    If you want 4-bit quantization for big models, set use_4bit=True (requires bitsandbytes).\n",
    "    \"\"\"\n",
    "    if model_id in _MODEL_CACHE:\n",
    "        return _MODEL_CACHE[model_id]\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "\n",
    "    model_kwargs = dict(\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    if use_4bit:\n",
    "        # requires bitsandbytes installed\n",
    "        model_kwargs.update(dict(load_in_4bit=True))\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, **model_kwargs)\n",
    "    model.eval()\n",
    "\n",
    "    _MODEL_CACHE[model_id] = (tok, model)\n",
    "    return tok, model\n",
    "\n",
    "\n",
    "def format_prompt(tokenizer, system_prompt: str, user_prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Prefer chat template if available; otherwise fall back to a simple structured prompt.\n",
    "    \"\"\"\n",
    "    sys_txt = (system_prompt or \"\").strip()\n",
    "    usr_txt = user_prompt.strip()\n",
    "\n",
    "    # Use tokenizer chat template if the model defines one\n",
    "    if hasattr(tokenizer, \"apply_chat_template\") and getattr(tokenizer, \"chat_template\", None):\n",
    "        messages = []\n",
    "        if sys_txt:\n",
    "            messages.append({\"role\": \"system\", \"content\": sys_txt})\n",
    "        messages.append({\"role\": \"user\", \"content\": usr_txt})\n",
    "        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    # Fallback prompt format (works “ok” for many instruction-ish models)\n",
    "    if sys_txt:\n",
    "        return f\"SYSTEM:\\n{sys_txt}\\n\\nUSER:\\n{usr_txt}\\n\\nASSISTANT:\\n\"\n",
    "    return f\"USER:\\n{usr_txt}\\n\\nASSISTANT:\\n\"\n",
    "\n",
    "\n",
    "def generation_kwargs_from_temp(temp: float) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    temp=0.0 => deterministic greedy\n",
    "    temp>0   => sampling\n",
    "    \"\"\"\n",
    "    if temp <= 0.0:\n",
    "        return dict(do_sample=False)\n",
    "    return dict(do_sample=True, temperature=float(temp), top_p=0.95)\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def hf_predict_one(\n",
    "    *,\n",
    "    model_id: str,\n",
    "    prompt: str,\n",
    "    system_prompt: str,\n",
    "    temperature: float,\n",
    "    max_new_tokens: int = 512,\n",
    "    use_4bit: bool = False,\n",
    ") -> str:\n",
    "    tok, model = load_hf_model(model_id, use_4bit=use_4bit)\n",
    "\n",
    "    full_user = (prompt.strip() + \"\\n\\n\" + ANSWER_INSTRUCTION).strip()\n",
    "    text = format_prompt(tok, system_prompt=system_prompt, user_prompt=full_user)\n",
    "\n",
    "    inputs = tok(text, return_tensors=\"pt\")\n",
    "    # move inputs to same device as model (device_map='auto' uses first param's device)\n",
    "    dev = next(model.parameters()).device\n",
    "    inputs = {k: v.to(dev) for k, v in inputs.items()}\n",
    "\n",
    "    gen_kwargs = {\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"eos_token_id\": tok.eos_token_id,\n",
    "        \"pad_token_id\": tok.eos_token_id,\n",
    "        **generation_kwargs_from_temp(temperature),\n",
    "    }\n",
    "\n",
    "    out = model.generate(**inputs, **gen_kwargs)\n",
    "    decoded = tok.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    # crude but effective: strip the prompt prefix if it’s echoed\n",
    "    if decoded.startswith(text):\n",
    "        decoded = decoded[len(text):]\n",
    "\n",
    "    return decoded.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b44fa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 — eval data builder (same shape as your RAG one)\n",
    "\n",
    "from typing import Optional, List, Dict, Any\n",
    "\n",
    "def build_eval_data_hf(\n",
    "    *,\n",
    "    dataset_rows: List[Dict[str, Any]],\n",
    "    model_id: str,\n",
    "    system_prompt: str,\n",
    "    temperature: float,\n",
    "    max_examples: Optional[int] = None,\n",
    "    max_new_tokens: int = 512,\n",
    "    use_4bit: bool = False,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    n = len(dataset_rows) if max_examples is None else min(len(dataset_rows), max_examples)\n",
    "    eval_data: List[Dict[str, Any]] = []\n",
    "\n",
    "    for i in range(n):\n",
    "        prompt = dataset_rows[i][\"prompt\"]\n",
    "        reference = dataset_rows[i][\"completion\"]\n",
    "\n",
    "        candidate = hf_predict_one(\n",
    "            model_id=model_id,\n",
    "            prompt=prompt,\n",
    "            system_prompt=system_prompt,\n",
    "            temperature=temperature,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            use_4bit=use_4bit,\n",
    "        )\n",
    "\n",
    "        eval_data.append({\n",
    "            \"inputs\": {\"prompt\": prompt},\n",
    "            \"outputs\": candidate,\n",
    "            \"expectations\": {\"expected_response\": reference},\n",
    "        })\n",
    "\n",
    "    return eval_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34beaadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 — traces -> per-example dataframe (reuse your proven helper)\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "def _maybe_json(x: Any) -> Any:\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip()\n",
    "        if (s.startswith(\"{\") and s.endswith(\"}\")) or (s.startswith(\"[\") and s.endswith(\"]\")):\n",
    "            try:\n",
    "                return json.loads(s)\n",
    "            except Exception:\n",
    "                return x\n",
    "    return x\n",
    "\n",
    "def traces_to_eval_df(traces: Any, judge_name: str) -> pd.DataFrame:\n",
    "    tdf = traces if isinstance(traces, pd.DataFrame) else pd.DataFrame(traces)\n",
    "    out_rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    for _, r in tdf.iterrows():\n",
    "        trace_id = r.get(\"trace_id\")\n",
    "\n",
    "        req = _maybe_json(r.get(\"request\"))\n",
    "        resp = _maybe_json(r.get(\"response\"))\n",
    "        exps = _maybe_json(r.get(\"expectations\"))\n",
    "        assessments = _maybe_json(r.get(\"assessments\"))\n",
    "\n",
    "        prompt = None\n",
    "        if isinstance(req, dict):\n",
    "            prompt = req.get(\"prompt\") or (req.get(\"inputs\") or {}).get(\"prompt\")\n",
    "\n",
    "        expected = None\n",
    "        if isinstance(exps, dict):\n",
    "            expected = exps.get(\"expected_response\") or exps.get(\"reference\") or exps.get(\"expected\")\n",
    "        elif isinstance(exps, str):\n",
    "            expected = exps\n",
    "\n",
    "        if expected is None and isinstance(req, dict):\n",
    "            exp_obj = req.get(\"expectations\")\n",
    "            if isinstance(exp_obj, dict):\n",
    "                expected = exp_obj.get(\"expected_response\") or exp_obj.get(\"reference\") or exp_obj.get(\"expected\")\n",
    "\n",
    "        if expected is None:\n",
    "            expected = r.get(\"expected_response\")\n",
    "\n",
    "        candidate = None\n",
    "        if isinstance(resp, str):\n",
    "            candidate = resp\n",
    "        elif isinstance(resp, dict):\n",
    "            candidate = resp.get(\"outputs\") or resp.get(\"response\") or resp.get(\"output\") or resp.get(\"text\")\n",
    "            if isinstance(candidate, dict):\n",
    "                candidate = candidate.get(\"response\") or candidate.get(\"text\") or str(candidate)\n",
    "\n",
    "        score = None\n",
    "        rationale = None\n",
    "        if isinstance(assessments, list):\n",
    "            for a in assessments:\n",
    "                if not isinstance(a, dict):\n",
    "                    continue\n",
    "                name = a.get(\"assessment_name\") or a.get(\"name\")\n",
    "                if name == judge_name:\n",
    "                    fb = a.get(\"feedback\") or {}\n",
    "                    score = fb.get(\"value\")\n",
    "                    rationale = a.get(\"rationale\") or a.get(\"explanation\")\n",
    "                    break\n",
    "\n",
    "        out_rows.append({\n",
    "            \"trace_id\": trace_id,\n",
    "            \"prompt\": prompt,\n",
    "            \"expected\": expected,\n",
    "            \"candidate\": candidate,\n",
    "            \"score\": score,\n",
    "            \"rationale\": rationale,\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(out_rows, columns=[\"trace_id\",\"prompt\",\"expected\",\"candidate\",\"score\",\"rationale\"])\n",
    "    df[\"score\"] = pd.to_numeric(df[\"score\"], errors=\"coerce\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9476c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 — run one HF config in MLflow (atomic)\n",
    "\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import mlflow\n",
    "\n",
    "def run_one_hf_cfg_mlflow(\n",
    "    *,\n",
    "    model_id: str,\n",
    "    temperature: float,\n",
    "    system_prompt_name: str,\n",
    "    system_prompt: str,\n",
    "    dataset_rows: List[Dict[str, Any]],\n",
    "    max_examples: int = 50,\n",
    "    max_new_tokens: int = 512,\n",
    "    use_4bit: bool = False,\n",
    "):\n",
    "    mlflow.log_params({\n",
    "        \"hf_model_id\": model_id,\n",
    "        \"temperature\": float(temperature),\n",
    "        \"system_prompt_variant\": system_prompt_name,\n",
    "        \"max_new_tokens\": int(max_new_tokens),\n",
    "        \"use_4bit\": bool(use_4bit),\n",
    "    })\n",
    "\n",
    "    eval_data = build_eval_data_hf(\n",
    "        dataset_rows=dataset_rows,\n",
    "        model_id=model_id,\n",
    "        system_prompt=system_prompt,\n",
    "        temperature=temperature,\n",
    "        max_examples=max_examples,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        use_4bit=use_4bit,\n",
    "    )\n",
    "\n",
    "    results = mlflow.genai.evaluate(data=eval_data, scorers=[gt_judge])\n",
    "\n",
    "    eval_run_id = results.run_id\n",
    "    mlflow.set_tag(\"eval_run_id\", eval_run_id)\n",
    "\n",
    "    traces = mlflow.search_traces(run_id=eval_run_id)\n",
    "    df = traces_to_eval_df(traces, judge_name=JUDGE_NAME)\n",
    "\n",
    "    # metrics\n",
    "    if (not df.empty) and df[\"score\"].notna().any():\n",
    "        mlflow.log_metric(\"judge_mean\", float(df[\"score\"].mean()))\n",
    "        mlflow.log_metric(\"judge_min\", float(df[\"score\"].min()))\n",
    "        mlflow.log_metric(\"judge_pass_rate_ge_0.8\", float((df[\"score\"] >= 0.8).mean()))\n",
    "        mlflow.log_metric(\"n_examples\", int(df[\"score\"].notna().sum()))\n",
    "    else:\n",
    "        mlflow.log_metric(\"judge_mean\", 0.0)\n",
    "        mlflow.log_metric(\"n_examples\", int(len(df)))\n",
    "\n",
    "    # artifacts\n",
    "    with tempfile.TemporaryDirectory() as td:\n",
    "        p = Path(td) / \"per_example_eval.csv\"\n",
    "        df.to_csv(p, index=False)\n",
    "        mlflow.log_artifact(str(p))\n",
    "        if hasattr(mlflow, \"log_table\"):\n",
    "            mlflow.log_table(df, \"per_example_eval_table.json\")\n",
    "\n",
    "    return df, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "facb8718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     24\u001b[39m run_name = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m|\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id.split(\u001b[33m'\u001b[39m\u001b[33m/\u001b[39m\u001b[33m'\u001b[39m)[-\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m|t=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m|4bit=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muse_4bit\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m mlflow.start_run(run_name=run_name, nested=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     df, _ = \u001b[43mrun_one_hf_cfg_mlflow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[43msystem_prompt_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpsys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataset_rows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_small\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# change to dataset_rows when ready\u001b[39;49;00m\n\u001b[32m     32\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_examples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# keep aligned with dataset_rows passed\u001b[39;49;00m\n\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_4bit\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDONE:\u001b[39m\u001b[33m\"\u001b[39m, run_name, \u001b[33m\"\u001b[39m\u001b[33mmean=\u001b[39m\u001b[33m\"\u001b[39m, df[\u001b[33m\"\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m\"\u001b[39m].mean())\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mrun_one_hf_cfg_mlflow\u001b[39m\u001b[34m(model_id, temperature, system_prompt_name, system_prompt, dataset_rows, max_examples, max_new_tokens, use_4bit)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_one_hf_cfg_mlflow\u001b[39m(\n\u001b[32m      8\u001b[39m     *,\n\u001b[32m      9\u001b[39m     model_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m     use_4bit: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     17\u001b[39m ):\n\u001b[32m     18\u001b[39m     mlflow.log_params({\n\u001b[32m     19\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhf_model_id\u001b[39m\u001b[33m\"\u001b[39m: model_id,\n\u001b[32m     20\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(temperature),\n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33muse_4bit\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mbool\u001b[39m(use_4bit),\n\u001b[32m     24\u001b[39m     })\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     eval_data = \u001b[43mbuild_eval_data_hf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataset_rows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_examples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_examples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_4bit\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m     results = mlflow.genai.evaluate(data=eval_data, scorers=[gt_judge])\n\u001b[32m     38\u001b[39m     eval_run_id = results.run_id\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mbuild_eval_data_hf\u001b[39m\u001b[34m(dataset_rows, model_id, system_prompt, temperature, max_examples, max_new_tokens, use_4bit)\u001b[39m\n\u001b[32m     19\u001b[39m     prompt = dataset_rows[i][\u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     20\u001b[39m     reference = dataset_rows[i][\u001b[33m\"\u001b[39m\u001b[33mcompletion\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     candidate = \u001b[43mhf_predict_one\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_4bit\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     eval_data.append({\n\u001b[32m     32\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33minputs\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m: prompt},\n\u001b[32m     33\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: candidate,\n\u001b[32m     34\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mexpectations\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mexpected_response\u001b[39m\u001b[33m\"\u001b[39m: reference},\n\u001b[32m     35\u001b[39m     })\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m eval_data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/TRACE/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 99\u001b[39m, in \u001b[36mhf_predict_one\u001b[39m\u001b[34m(model_id, prompt, system_prompt, temperature, max_new_tokens, use_4bit)\u001b[39m\n\u001b[32m     90\u001b[39m inputs = {k: v.to(dev) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs.items()}\n\u001b[32m     92\u001b[39m gen_kwargs = {\n\u001b[32m     93\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmax_new_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_new_tokens,\n\u001b[32m     94\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33meos_token_id\u001b[39m\u001b[33m\"\u001b[39m: tok.eos_token_id,\n\u001b[32m     95\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpad_token_id\u001b[39m\u001b[33m\"\u001b[39m: tok.eos_token_id,\n\u001b[32m     96\u001b[39m     **generation_kwargs_from_temp(temperature),\n\u001b[32m     97\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m out = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m decoded = tok.decode(out[\u001b[32m0\u001b[39m], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    102\u001b[39m \u001b[38;5;66;03m# crude but effective: strip the prompt prefix if it’s echoed\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/TRACE/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/TRACE/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/TRACE/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:2787\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2785\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2786\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2787\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2789\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   2790\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   2791\u001b[39m     outputs,\n\u001b[32m   2792\u001b[39m     model_kwargs,\n\u001b[32m   2793\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2794\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/TRACE/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/TRACE/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/TRACE/.venv/lib/python3.11/site-packages/transformers/utils/generic.py:918\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    920\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/TRACE/.venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:459\u001b[39m, in \u001b[36mLlamaForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m    440\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    441\u001b[39m ) -> CausalLMOutputWithPast:\n\u001b[32m    442\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    443\u001b[39m \u001b[33;03m    Example:\u001b[39;00m\n\u001b[32m    444\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    457\u001b[39m \u001b[33;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[32m    458\u001b[39m \u001b[33;03m    ```\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m     outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    470\u001b[39m     hidden_states = outputs.last_hidden_state\n\u001b[32m    471\u001b[39m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/TRACE/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/TRACE/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/TRACE/.venv/lib/python3.11/site-packages/transformers/utils/generic.py:1072\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1069\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m   1071\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1072\u001b[39m     outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1074\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1075\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1076\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1077\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/TRACE/.venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:395\u001b[39m, in \u001b[36mLlamaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, cache_position, use_cache, **kwargs)\u001b[39m\n\u001b[32m    392\u001b[39m position_embeddings = \u001b[38;5;28mself\u001b[39m.rotary_emb(hidden_states, position_ids)\n\u001b[32m    394\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers[: \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers]:\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m     hidden_states = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    405\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.norm(hidden_states)\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[32m    407\u001b[39m     last_hidden_state=hidden_states,\n\u001b[32m    408\u001b[39m     past_key_values=past_key_values,\n\u001b[32m    409\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/TRACE/.venv/lib/python3.11/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/TRACE/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/TRACE/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/TRACE/.venv/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/TRACE/.venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:309\u001b[39m, in \u001b[36mLlamaDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    307\u001b[39m residual = hidden_states\n\u001b[32m    308\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.post_attention_layernorm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    310\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/TRACE/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/TRACE/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/TRACE/.venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155\u001b[39m, in \u001b[36mLlamaMLP.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m     down_proj = \u001b[38;5;28mself\u001b[39m.down_proj(\u001b[38;5;28mself\u001b[39m.act_fn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) * \u001b[38;5;28mself\u001b[39m.up_proj(x))\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/TRACE/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/TRACE/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/TRACE/.venv/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:569\u001b[39m, in \u001b[36mLinear4bit.forward\u001b[39m\u001b[34m(self, x, *args, **kwargs)\u001b[39m\n\u001b[32m    566\u001b[39m     x = \u001b[38;5;28mself\u001b[39m._cast_input_dtype(x, lora_A.weight.dtype)\n\u001b[32m    568\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m active_adapter \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.lora_variant:  \u001b[38;5;66;03m# vanilla LoRA\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m569\u001b[39m     output = \u001b[43mlora_B\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlora_A\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m * scaling\n\u001b[32m    570\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m requires_conversion:\n\u001b[32m    571\u001b[39m         output = output.to(expected_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/TRACE/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/TRACE/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/TRACE/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Cell 8 — sweep the two HF models × temps × system prompts\n",
    "\n",
    "import random\n",
    "\n",
    "HF_MODELS = [\n",
    "    \"pesimachete/cisco-iosxr-telemetry-model_3B\"\n",
    "    # \"pesimachete/cisco-iosxr-telemetry-model_8B\",\n",
    "]\n",
    "TEMPS = [0.0]\n",
    "\n",
    "# Quick smoke test sample (optional)\n",
    "random.seed(42)\n",
    "dataset_small = random.sample(dataset_rows, 1)\n",
    "\n",
    "# For 8B: if you might be tight on VRAM, set use_4bit=True (and install bitsandbytes)\n",
    "USE_4BIT_FOR_8B = False\n",
    "\n",
    "with mlflow.start_run(run_name=\"parent_sweep_hf\") as parent:\n",
    "    for (pname, psys) in PROMPT_VARIANTS:\n",
    "        for model_id in HF_MODELS:\n",
    "            for t in TEMPS:\n",
    "                use_4bit = (USE_4BIT_FOR_8B and model_id.endswith(\"_8B\"))\n",
    "\n",
    "                run_name = f\"{pname}|{model_id.split('/')[-1]}|t={t}|4bit={use_4bit}\"\n",
    "                with mlflow.start_run(run_name=run_name, nested=True):\n",
    "                    df, _ = run_one_hf_cfg_mlflow(\n",
    "                        model_id=model_id,\n",
    "                        temperature=t,\n",
    "                        system_prompt_name=pname,\n",
    "                        system_prompt=psys,\n",
    "                        dataset_rows=dataset_small,   # change to dataset_rows when ready\n",
    "                        max_examples=50,              # keep aligned with dataset_rows passed\n",
    "                        max_new_tokens=512,\n",
    "                        use_4bit=use_4bit,\n",
    "                    )\n",
    "                    print(\"DONE:\", run_name, \"mean=\", df[\"score\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13fc00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote per_example_all_hf_runs_from_traces.csv rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Cell 9 — collect per-example results from all child runs into one CSV (like your RAG version)\n",
    "\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "\n",
    "exp = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "runs_df = mlflow.search_runs(experiment_ids=[exp.experiment_id], output_format=\"pandas\")\n",
    "child = runs_df[runs_df[\"tags.mlflow.runName\"] != \"parent_sweep_hf\"].copy()\n",
    "\n",
    "all_dfs = []\n",
    "for _, rr in child.iterrows():\n",
    "    eval_run_id = rr.get(\"tags.eval_run_id\")\n",
    "    if pd.isna(eval_run_id) or eval_run_id is None:\n",
    "        eval_run_id = rr[\"run_id\"]\n",
    "\n",
    "    traces = mlflow.search_traces(run_id=str(eval_run_id))\n",
    "    df = traces_to_eval_df(traces, judge_name=JUDGE_NAME)\n",
    "\n",
    "    df[\"run_id\"] = rr[\"run_id\"]\n",
    "    df[\"run_name\"] = rr.get(\"tags.mlflow.runName\")\n",
    "    df[\"hf_model_id\"] = rr.get(\"params.hf_model_id\")\n",
    "    df[\"temperature\"] = rr.get(\"params.temperature\")\n",
    "    df[\"system_prompt_variant\"] = rr.get(\"params.system_prompt_variant\")\n",
    "    df[\"use_4bit\"] = rr.get(\"params.use_4bit\")\n",
    "\n",
    "    all_dfs.append(df)\n",
    "\n",
    "per_example_all = pd.concat(all_dfs, ignore_index=True) if all_dfs else pd.DataFrame()\n",
    "per_example_all.to_csv(\"per_example_all_hf_runs_from_traces.csv\", index=False)\n",
    "print(\"Wrote per_example_all_hf_runs_from_traces.csv rows:\", len(per_example_all))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
