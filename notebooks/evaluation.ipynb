{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c026f472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is set: True\n",
      "Length: 164\n",
      "Prefix: sk-proj...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "key = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(\"OPENAI_API_KEY is set:\", bool(key))\n",
    "print(\"Length:\", len(key) if key else None)\n",
    "print(\"Prefix:\", key[:7] + \"...\" if key else None)  # optional: shows only first chars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4c32f305",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.genai.judges import make_judge\n",
    "\n",
    "gt_judge = make_judge(\n",
    "    name=\"xr_config_quality_score\",\n",
    "    instructions=(\n",
    "        \"You are evaluating IOS XR telemetry config generation.\\n\\n\"\n",
    "        \"inputs:\\n{{ inputs }}\\n\\n\"\n",
    "        \"output:\\n{{ outputs }}\\n\\n\"\n",
    "        \"expected:\\n{{ expectations }}\\n\\n\"\n",
    "        \"Return ONLY a single floating point number between 0.0 and 1.0.\\n\"\n",
    "        \"No words, no labels, no JSON, no explanation.\\n\\n\"\n",
    "        \"Scoring rubric (lenient):\\n\"\n",
    "        \"- Ignore group/subscription names and ordering.\\n\"\n",
    "        \"- Ignore sample-interval unless user explicitly requested it.\\n\"\n",
    "        \"- Be strict about user-specified IP/port/protocol/encoding.\\n\"\n",
    "        \"- Penalize sensor-paths only if clearly unrelated.\\n\"\n",
    "    ),\n",
    "    feedback_value_type=float,\n",
    "    model=\"openai:/gpt-4.1-mini\",\n",
    "    inference_params={\"temperature\": 0, \"max_tokens\": 10},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dae8bb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class RunCfg:\n",
    "    vector_db: str\n",
    "    top_k: int\n",
    "    filter_fields: Dict[str, Any]\n",
    "    temperature: float\n",
    "    model_chat: str\n",
    "    model_embed: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "525669bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,\n",
       " RunCfg(vector_db='catalog_embeddings', top_k=5, filter_fields={}, temperature=0.1, model_chat='gpt-4.1-mini', model_embed='text-embedding-3-small'))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_grid(vector_dbs, top_ks, filters, temps, chat_models, embed_models):\n",
    "    cfgs = []\n",
    "    for vdb in vector_dbs:\n",
    "        for k in top_ks:\n",
    "            for ff in filters:\n",
    "                for t in temps:\n",
    "                    for cm in chat_models:\n",
    "                        for em in embed_models:\n",
    "                            cfgs.append(RunCfg(vdb, k, ff, t, cm, em))\n",
    "    return cfgs\n",
    "\n",
    "VECTOR_DBS = [\"catalog_embeddings\", \"fixed_window_embeddings\"]  # add your 3rd\n",
    "TOP_KS = [5, 10]\n",
    "FILTERS = [{}]  # adjust to your payload schema,  {\"protocol_tag\": \"bgp\"}\n",
    "TEMPS = [0.1]\n",
    "CHAT_MODELS = [\"gpt-4.1-mini\"]\n",
    "EMBED_MODELS = [\"text-embedding-3-small\"]\n",
    "\n",
    "cfgs = make_grid(VECTOR_DBS, TOP_KS, FILTERS, TEMPS, CHAT_MODELS, EMBED_MODELS)\n",
    "len(cfgs), cfgs[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5739fc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd().resolve().parents[0]  # repo root if cwd==repo/notebooks\n",
    "sys.path.insert(0, str(ROOT / \"src\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e0b15d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported tracerag modules OK.\n"
     ]
    }
   ],
   "source": [
    "from tracerag.rag.naive import naive_rag, build_openai_chat_fn\n",
    "from tracerag.retrieval.qdrant import (\n",
    "    QdrantRetrievalConfig,\n",
    "    build_openai_embedding_fn,\n",
    "    build_qdrant_retriever,\n",
    ")\n",
    "\n",
    "print(\"Imported tracerag modules OK.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c0e2e1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class RunCfg:\n",
    "    vector_db: str                 # we'll map this to a Qdrant collection (or backend)\n",
    "    top_k: int\n",
    "    filter_fields: Dict[str, Any]  # e.g., {\"domain\":\"bgp\"} or {}\n",
    "    temperature: float\n",
    "    model_chat: str\n",
    "    model_embed: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f921b6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models as qmodels\n",
    "\n",
    "from typing import Any, Dict, List, Optional, Sequence, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c6b15f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_qdrant_filter(filter_fields: Dict[str, Any]) -> Optional[qmodels.Filter]:\n",
    "    if not filter_fields:\n",
    "        return None\n",
    "\n",
    "    must: List[qmodels.FieldCondition] = []\n",
    "    for key, value in filter_fields.items():\n",
    "        must.append(\n",
    "            qmodels.FieldCondition(\n",
    "                key=key,\n",
    "                match=qmodels.MatchValue(value=value),\n",
    "            )\n",
    "        )\n",
    "    return qmodels.Filter(must=must)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8dcb15dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_retriever(\n",
    "    *,\n",
    "    cfg: RunCfg,\n",
    "    qdrant: QdrantClient,\n",
    "    openai_client: OpenAI,\n",
    "):\n",
    "    embed_fn = build_openai_embedding_fn(openai_client, model=cfg.model_embed)\n",
    "\n",
    "    q_filter = build_qdrant_filter(cfg.filter_fields)\n",
    "\n",
    "    config = QdrantRetrievalConfig(\n",
    "        collection_name=cfg.vector_db,  # <-- mapping: vector_db -> collection\n",
    "        top_k=cfg.top_k,\n",
    "        query_filter=q_filter,\n",
    "    )\n",
    "\n",
    "    qdrant_retriever = build_qdrant_retriever(\n",
    "        qdrant=qdrant,\n",
    "        embedding_fn=embed_fn,\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    # Return a function: retriever(query, k) -> List[Chunk]\n",
    "    # Keep filter fixed per cfg (since filter_fields is a variation)\n",
    "    return lambda query, k: qdrant_retriever(query, top_k=k, query_filter=q_filter)\n",
    "\n",
    "\n",
    "def make_chat_fn(cfg: RunCfg, openai_client: OpenAI):\n",
    "    return build_openai_chat_fn(\n",
    "        openai_client,\n",
    "        model=cfg.model_chat,\n",
    "        temperature=cfg.temperature,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "692acef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System prompt length: 1573\n",
      "You are a Cisco IOS XR network engineer generating IOS XR 7.x model-driven telemetry configuration.\n",
      "\n",
      "INPUTS:\n",
      "- USER_REQUEST: describes the intent (e.g., “BGP telemetry”), destination IP/port, and optionally interval.\n",
      "- CONTEXT: a list of valid YANG sensor-path candidates.\n",
      "\n",
      "HARD RULES:\n",
      "- Output ONLY \n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT_PATH = Path(\"../data/iosxr_prompt.txt\")  # <-- your file\n",
    "SYSTEM_PROMPT = SYSTEM_PROMPT_PATH.read_text(encoding=\"utf-8\")\n",
    "\n",
    "print(\"System prompt length:\", len(SYSTEM_PROMPT))\n",
    "print(SYSTEM_PROMPT[:300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5c4dbb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_predict_one(prompt: str, cfg: RunCfg, qdrant, openai_client) -> str:\n",
    "    retriever = make_retriever(cfg=cfg, qdrant=qdrant, openai_client=openai_client)\n",
    "    chat_fn = make_chat_fn(cfg, openai_client=openai_client)\n",
    "\n",
    "    resp = naive_rag(\n",
    "        user_query=prompt,\n",
    "        retriever=retriever,\n",
    "        chat_fn=chat_fn,\n",
    "        system_prompt=SYSTEM_PROMPT,\n",
    "        top_k=cfg.top_k,\n",
    "        answer_instruction=\"Return only IOS XR telemetry configuration.\",\n",
    "    )\n",
    "    return resp.answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4ddb08ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_eval_data(dataset_rows, cfg: RunCfg, qdrant, openai_client, max_examples=None):\n",
    "    n = len(dataset_rows) if max_examples is None else min(len(dataset_rows), max_examples)\n",
    "    eval_data = []\n",
    "\n",
    "    for i in range(n):\n",
    "        prompt = dataset_rows[i][\"prompt\"]\n",
    "        reference = dataset_rows[i][\"completion\"]\n",
    "\n",
    "        candidate = rag_predict_one(prompt, cfg, qdrant, openai_client)\n",
    "\n",
    "        eval_data.append({\n",
    "            \"inputs\": {\"prompt\": prompt},\n",
    "            \"outputs\": candidate,\n",
    "            \"expectations\": {\"expected_response\": reference},\n",
    "        })\n",
    "    return eval_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9c288a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import mlflow\n",
    "\n",
    "def run_one_cfg_mlflow(cfg: RunCfg, dataset_rows, qdrant, openai_client, max_examples=None):\n",
    "    # log only your 6 variations\n",
    "    mlflow.log_params({\n",
    "        \"vector_db\": cfg.vector_db,\n",
    "        \"top_k\": cfg.top_k,\n",
    "        \"filter_fields\": json.dumps(cfg.filter_fields, sort_keys=True),\n",
    "        \"temperature\": cfg.temperature,\n",
    "        \"model_chat\": cfg.model_chat,\n",
    "        \"model_embed\": cfg.model_embed,\n",
    "    })\n",
    "\n",
    "    eval_data = build_eval_data(dataset_rows, cfg, qdrant, openai_client, max_examples=max_examples)\n",
    "\n",
    "    # MLflow judge\n",
    "    results = mlflow.genai.evaluate(\n",
    "        data=eval_data,\n",
    "        scorers=[gt_judge],\n",
    "    )\n",
    "\n",
    "    # results.metrics usually contains aggregate metrics; but easiest is to also compute pass rate from the table\n",
    "    # MLflow returns an EvaluationResult with a \"tables\" field in many setups.\n",
    "    # We'll be defensive and compute ourselves:\n",
    "    # Each row's result is stored in results.tables[\"evaluation_results\"] or similar depending on version.\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2c41832f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded rows: 10\n",
      "Example keys: dict_keys(['prompt', 'completion'])\n",
      "Prompt preview:\n",
      " Generate Cisco IOS XR 7.0.1 telemetry configuration to monitor L2VPN xconnect/pseudowire operational state. Use gRPC with no TLS. Telemetry server address is 192.0.2.110 with port 57500. Choose releva\n",
      "Completion preview:\n",
      " telemetry model-driven\n",
      " sensor-group L2VPN-XCONNECT-OPER\n",
      " sensor-path Cisco-IOS-XR-l2vpn-oper:l2vpnv2/active/xconnects\n",
      " sensor-path Cisco-IOS-XR-l2vpn-oper:l2vpnv2/active/pseudowires\n",
      " sensor-path Cisc\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = Path(\"../data/judge_dataset.jsonl\")  # <-- change if needed\n",
    "\n",
    "def load_jsonl(path: Path) -> List[Dict[str, Any]]:\n",
    "    rows = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "dataset = load_jsonl(DATASET_PATH)\n",
    "print(\"Loaded rows:\", len(dataset))\n",
    "print(\"Example keys:\", dataset[0].keys())\n",
    "print(\"Prompt preview:\\n\", dataset[0][\"prompt\"][:200])\n",
    "print(\"Completion preview:\\n\", dataset[0][\"completion\"][:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "29379b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "dataset = random.sample(dataset, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b40b6f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prompt': 'Create telemetry model-driven configuration for Cisco IOS XR 7.0.1 to stream routing information base (RIB) summary and route counters (IPv4/IPv6). Use gRPC with no TLS. Telemetry server is 192.0.2.111 port 57500. Choose relevant sensor-paths.',\n",
       "  'completion': 'telemetry model-driven\\n sensor-group RIB-OPER\\n sensor-path Cisco-IOS-XR-rib-oper:rib/vrfs/vrf/afi-safi/ipv4-unicast\\n sensor-path Cisco-IOS-XR-rib-oper:rib/vrfs/vrf/afi-safi/ipv6-unicast\\n sensor-path Cisco-IOS-XR-rib-oper:rib/summary\\n !\\n destination-group DG-RIB\\n address-family ipv4\\n destination 192.0.2.111\\n port 57500\\n encoding self-describing-gpb\\n protocol grpc no-tls\\n !\\n subscription SUB-RIB\\n sensor-group-id RIB-OPER sample-interval 60000\\n destination-group-id DG-RIB\\n !\\n!'},\n",
       " {'prompt': 'Generate Cisco IOS XR 7.0.1 telemetry configuration to monitor L2VPN xconnect/pseudowire operational state. Use gRPC with no TLS. Telemetry server address is 192.0.2.110 with port 57500. Choose relevant sensor-paths.',\n",
       "  'completion': 'telemetry model-driven\\n sensor-group L2VPN-XCONNECT-OPER\\n sensor-path Cisco-IOS-XR-l2vpn-oper:l2vpnv2/active/xconnects\\n sensor-path Cisco-IOS-XR-l2vpn-oper:l2vpnv2/active/pseudowires\\n sensor-path Cisco-IOS-XR-l2vpn-oper:l2vpnv2/active/bridge-domains\\n !\\n destination-group DG-L2VPN\\n address-family ipv4\\n destination 192.0.2.110\\n port 57500\\n encoding self-describing-gpb\\n protocol grpc no-tls\\n !\\n subscription SUB-L2VPN\\n sensor-group-id L2VPN-XCONNECT-OPER sample-interval 60000\\n destination-group-id DG-L2VPN\\n !\\n!'},\n",
       " {'prompt': 'Generate Cisco IOS XR 7.0.1 telemetry configuration for ACL operational counters (hit counts per ACE and interface bindings). Use gRPC with no TLS. Server 192.0.2.114 port 57500. Choose relevant sensor-paths.',\n",
       "  'completion': 'telemetry model-driven\\n sensor-group ACL-OPER\\n sensor-path Cisco-IOS-XR-acl-oper:acl/active\\n sensor-path Cisco-IOS-XR-acl-oper:acl/active/vrfs/vrf\\n sensor-path Cisco-IOS-XR-acl-oper:acl/active/interfaces/interface\\n !\\n destination-group DG-ACL\\n address-family ipv4\\n destination 192.0.2.114\\n port 57500\\n encoding self-describing-gpb\\n protocol grpc no-tls\\n !\\n subscription SUB-ACL\\n sensor-group-id ACL-OPER sample-interval 60000\\n destination-group-id DG-ACL\\n !\\n!'},\n",
       " {'prompt': 'Create Cisco IOS XR 7.0.1 telemetry model-driven configuration to monitor CPU and memory operational utilization (processes and summary). Use gRPC with no TLS. Telemetry server 192.0.2.119 port 57500. Choose relevant sensor-paths.',\n",
       "  'completion': 'telemetry model-driven\\n sensor-group SYS-RESOURCES\\n sensor-path Cisco-IOS-XR-procfs-oper:processes\\n sensor-path Cisco-IOS-XR-wdsysmon-fd-oper:system-monitoring/cpu-utilization\\n sensor-path Cisco-IOS-XR-wdsysmon-fd-oper:system-monitoring/memory-utilization\\n !\\n destination-group DG-SYS\\n address-family ipv4\\n destination 192.0.2.119\\n port 57500\\n encoding self-describing-gpb\\n protocol grpc no-tls\\n !\\n subscription SUB-SYS\\n sensor-group-id SYS-RESOURCES sample-interval 30000\\n destination-group-id DG-SYS\\n !\\n!'},\n",
       " {'prompt': 'Generate Cisco IOS XR 7.0.1 telemetry configuration for multicast (PIM/IGMP) operational monitoring (neighbors, groups, interfaces). Use gRPC with no TLS. Server 192.0.2.116 port 57500. Choose relevant sensor-paths.',\n",
       "  'completion': 'telemetry model-driven\\n sensor-group MCAST-OPER\\n sensor-path Cisco-IOS-XR-ipv4-igmp-oper:igmp/active\\n sensor-path Cisco-IOS-XR-ipv4-pim-oper:pim/active\\n sensor-path Cisco-IOS-XR-ipv4-pim-oper:pim/active/vrfs/vrf/interfaces\\n !\\n destination-group DG-MCAST\\n address-family ipv4\\n destination 192.0.2.116\\n port 57500\\n encoding self-describing-gpb\\n protocol grpc no-tls\\n !\\n subscription SUB-MCAST\\n sensor-group-id MCAST-OPER sample-interval 60000\\n destination-group-id DG-MCAST\\n !\\n!'},\n",
       " {'prompt': 'Create Cisco IOS XR 7.0.1 telemetry model-driven configuration to monitor NetFlow/flow monitor operational statistics (exporter, cache, drops). Use gRPC with no TLS. Telemetry server 192.0.2.115 port 57500. Choose relevant sensor-paths.',\n",
       "  'completion': 'telemetry model-driven\\n sensor-group NETFLOW-OPER\\n sensor-path Cisco-IOS-XR-flowspec-oper:flow\\n sensor-path Cisco-IOS-XR-netflow-oper:net-flow/nodes/node\\n sensor-path Cisco-IOS-XR-netflow-oper:net-flow/nodes/node/exporters\\n !\\n destination-group DG-NETFLOW\\n address-family ipv4\\n destination 192.0.2.115\\n port 57500\\n encoding self-describing-gpb\\n protocol grpc no-tls\\n !\\n subscription SUB-NETFLOW\\n sensor-group-id NETFLOW-OPER sample-interval 60000\\n destination-group-id DG-NETFLOW\\n !\\n!'},\n",
       " {'prompt': 'Generate Cisco IOS XR 7.0.1 telemetry configuration for LACP/EtherChannel operational monitoring (bundles, members, counters). Use gRPC with no TLS. Telemetry server is 192.0.2.118 on port 57500. Choose relevant sensor-paths.',\n",
       "  'completion': 'telemetry model-driven\\n sensor-group LACP-OPER\\n sensor-path Cisco-IOS-XR-bundlemgr-oper:bundle-information\\n sensor-path Cisco-IOS-XR-lacp-oper:lacp/nodes/node/bundles\\n sensor-path Cisco-IOS-XR-lacp-oper:lacp/nodes/node/interfaces\\n !\\n destination-group DG-LACP\\n address-family ipv4\\n destination 192.0.2.118\\n port 57500\\n encoding self-describing-gpb\\n protocol grpc no-tls\\n !\\n subscription SUB-LACP\\n sensor-group-id LACP-OPER sample-interval 30000\\n destination-group-id DG-LACP\\n !\\n!'},\n",
       " {'prompt': 'Generate Cisco IOS XR 7.0.1 telemetry configuration for CEF/FIB operational monitoring (forwarding entries and statistics). Use gRPC with no TLS. Telemetry server 192.0.2.112 port 57500. Choose relevant sensor-paths.',\n",
       "  'completion': 'telemetry model-driven\\n sensor-group FIB-OPER\\n sensor-path Cisco-IOS-XR-fib-common-oper:fib/nodes/node/protocols\\n sensor-path Cisco-IOS-XR-fib-common-oper:fib/nodes/node/summary\\n sensor-path Cisco-IOS-XR-fib-common-oper:fib/nodes/node/statistics\\n !\\n destination-group DG-FIB\\n address-family ipv4\\n destination 192.0.2.112\\n port 57500\\n encoding self-describing-gpb\\n protocol grpc no-tls\\n !\\n subscription SUB-FIB\\n sensor-group-id FIB-OPER sample-interval 60000\\n destination-group-id DG-FIB\\n !\\n!'},\n",
       " {'prompt': 'Create Cisco IOS XR 7.0.1 telemetry model-driven configuration to monitor QoS policy-map statistics (interfaces, class counters, drops). Use gRPC with no TLS. Telemetry server is 192.0.2.113 on port 57500. Choose relevant sensor-paths.',\n",
       "  'completion': 'telemetry model-driven\\n sensor-group QOS-OPER\\n sensor-path Cisco-IOS-XR-qos-ma-oper:qos/nodes/node/policy-map\\n sensor-path Cisco-IOS-XR-qos-ma-oper:qos/nodes/node/interfaces/interface\\n sensor-path Cisco-IOS-XR-qos-ma-oper:qos/nodes/node/class-map\\n !\\n destination-group DG-QOS\\n address-family ipv4\\n destination 192.0.2.113\\n port 57500\\n encoding self-describing-gpb\\n protocol grpc no-tls\\n !\\n subscription SUB-QOS\\n sensor-group-id QOS-OPER sample-interval 30000\\n destination-group-id DG-QOS\\n !\\n!'},\n",
       " {'prompt': 'Create Cisco IOS XR 7.0.1 telemetry model-driven configuration for VRF operational monitoring (VRF state and route-targets). Use gRPC with no TLS. Telemetry server 192.0.2.117 port 57500. Choose relevant sensor-paths.',\n",
       "  'completion': 'telemetry model-driven\\n sensor-group VRF-OPER\\n sensor-path Cisco-IOS-XR-infra-rsi-oper:rsi/vrfs/vrf\\n sensor-path Cisco-IOS-XR-infra-rsi-oper:rsi/vrfs/vrf/afs/af\\n sensor-path Cisco-IOS-XR-infra-rsi-oper:rsi/vrfs/vrf/route-targets\\n !\\n destination-group DG-VRF\\n address-family ipv4\\n destination 192.0.2.117\\n port 57500\\n encoding self-describing-gpb\\n protocol grpc no-tls\\n !\\n subscription SUB-VRF\\n sensor-group-id VRF-OPER sample-interval 120000\\n destination-group-id DG-VRF\\n !\\n!'}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6b43aa3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/10 [Elapsed: 00:00, Remaining: ?] 2026/01/11 23:33:33 WARNING mlflow.tracing.export.mlflow_v3: Failed to send trace to MLflow backend: Yaml file '/home/musel/Documents/github/TRACE/mlruns/332362951122688275/traces/tr-01a9e71fde8a774bcf36d58b47378190/trace_info.yaml' exists as '/home/musel/Documents/github/TRACE/mlruns/332362951122688275/traces/tr-01a9e71fde8a774bcf36d58b47378190/trace_info.yaml\n",
      "2026/01/11 23:33:33 WARNING mlflow.tracing.export.mlflow_v3: Failed to send trace to MLflow backend: Yaml file '/home/musel/Documents/github/TRACE/mlruns/332362951122688275/traces/tr-bacfb3d00b1f9163ce9ff57f43b7a3a6/trace_info.yaml' exists as '/home/musel/Documents/github/TRACE/mlruns/332362951122688275/traces/tr-bacfb3d00b1f9163ce9ff57f43b7a3a6/trace_info.yaml\n",
      "2026/01/11 23:33:33 WARNING mlflow.tracing.export.mlflow_v3: Failed to send trace to MLflow backend: Yaml file '/home/musel/Documents/github/TRACE/mlruns/332362951122688275/traces/tr-386ecbe06b65a6a48b8148f6b38a088c/trace_info.yaml' exists as '/home/musel/Documents/github/TRACE/mlruns/332362951122688275/traces/tr-386ecbe06b65a6a48b8148f6b38a088c/trace_info.yaml\n",
      "2026/01/11 23:33:33 WARNING mlflow.tracing.export.mlflow_v3: Failed to send trace to MLflow backend: Yaml file '/home/musel/Documents/github/TRACE/mlruns/332362951122688275/traces/tr-571aa8766c307511b2b9437a28df6ec4/trace_info.yaml' exists as '/home/musel/Documents/github/TRACE/mlruns/332362951122688275/traces/tr-571aa8766c307511b2b9437a28df6ec4/trace_info.yaml\n",
      "2026/01/11 23:33:33 WARNING mlflow.tracing.export.mlflow_v3: Failed to send trace to MLflow backend: Yaml file '/home/musel/Documents/github/TRACE/mlruns/332362951122688275/traces/tr-d8f564135be6128e18c267976142ea7d/trace_info.yaml' exists as '/home/musel/Documents/github/TRACE/mlruns/332362951122688275/traces/tr-d8f564135be6128e18c267976142ea7d/trace_info.yaml\n",
      "2026/01/11 23:33:33 WARNING mlflow.tracing.export.mlflow_v3: Failed to send trace to MLflow backend: Yaml file '/home/musel/Documents/github/TRACE/mlruns/332362951122688275/traces/tr-562b0f79c37459eef50bea63371ecd7b/trace_info.yaml' exists as '/home/musel/Documents/github/TRACE/mlruns/332362951122688275/traces/tr-562b0f79c37459eef50bea63371ecd7b/trace_info.yaml\n",
      "2026/01/11 23:33:33 WARNING mlflow.tracing.export.mlflow_v3: Failed to send trace to MLflow backend: Yaml file '/home/musel/Documents/github/TRACE/mlruns/332362951122688275/traces/tr-9e574f7aa0ee89aed453dd324b0dbb41/trace_info.yaml' exists as '/home/musel/Documents/github/TRACE/mlruns/332362951122688275/traces/tr-9e574f7aa0ee89aed453dd324b0dbb41/trace_info.yaml\n",
      "2026/01/11 23:33:33 WARNING mlflow.tracing.export.mlflow_v3: Failed to send trace to MLflow backend: Yaml file '/home/musel/Documents/github/TRACE/mlruns/332362951122688275/traces/tr-37f8a88b17fc695a07a0ca6e0822e8f3/trace_info.yaml' exists as '/home/musel/Documents/github/TRACE/mlruns/332362951122688275/traces/tr-37f8a88b17fc695a07a0ca6e0822e8f3/trace_info.yaml\n",
      "2026/01/11 23:33:33 WARNING mlflow.tracing.export.mlflow_v3: Failed to send trace to MLflow backend: Yaml file '/home/musel/Documents/github/TRACE/mlruns/332362951122688275/traces/tr-32e706298fadc1a606cb0fb39a1de644/trace_info.yaml' exists as '/home/musel/Documents/github/TRACE/mlruns/332362951122688275/traces/tr-32e706298fadc1a606cb0fb39a1de644/trace_info.yaml\n",
      "2026/01/11 23:33:33 WARNING mlflow.tracing.export.mlflow_v3: Failed to send trace to MLflow backend: Yaml file '/home/musel/Documents/github/TRACE/mlruns/332362951122688275/traces/tr-60e7a113ec1b8ca1f91e1d4c1ff49b78/trace_info.yaml' exists as '/home/musel/Documents/github/TRACE/mlruns/332362951122688275/traces/tr-60e7a113ec1b8ca1f91e1d4c1ff49b78/trace_info.yaml\n",
      "Evaluating: 100%|██████████| 10/10 [Elapsed: 00:00, Remaining: 00:00] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✨ Evaluation completed.\n",
      "\n",
      "Metrics and evaluation results are logged to the MLflow run:\n",
      "  Run name: \u001b[94mcatalog_embeddings|k=5|t=0.1|chat=gpt-4.1-mini|emb=text-embedding-3-small|f={}\u001b[0m\n",
      "  Run ID: \u001b[94m290b9c4438ea4f31916d95a4a9834a79\u001b[0m\n",
      "\n",
      "To view the detailed evaluation results with sample-wise scores,\n",
      "open the \u001b[93m\u001b[1mTraces\u001b[0m tab in the Run page in the MLflow UI.\n",
      "\n",
      "DONE: catalog_embeddings|k=5|t=0.1|chat=gpt-4.1-mini|emb=text-embedding-3-small|f={}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/11 23:34:10 WARNING mlflow.tracing.export.mlflow_v3: Failed to send trace to MLflow backend: Yaml file '/home/musel/Documents/github/TRACE/mlruns/332362951122688275/traces/tr-3602f8ac10f1bc81448aaa9e66b2bc5b/trace_info.yaml' exists as '/home/musel/Documents/github/TRACE/mlruns/332362951122688275/traces/tr-3602f8ac10f1bc81448aaa9e66b2bc5b/trace_info.yaml\n",
      "2026/01/11 23:34:10 WARNING mlflow.tracing.export.mlflow_v3: Failed to send trace to MLflow backend: Yaml file '/home/musel/Documents/github/TRACE/mlruns/332362951122688275/traces/tr-508ebad7b7c93acfe059a0ee9132b63e/trace_info.yaml' exists as '/home/musel/Documents/github/TRACE/mlruns/332362951122688275/traces/tr-508ebad7b7c93acfe059a0ee9132b63e/trace_info.yaml\n",
      "2026/01/11 23:34:10 WARNING mlflow.tracing.export.mlflow_v3: Failed to send trace to MLflow backend: Yaml file '/home/musel/Documents/github/TRACE/mlruns/332362951122688275/traces/tr-ea1fca65e27a984d654821d07fcd9eb1/trace_info.yaml' exists as '/home/musel/Documents/github/TRACE/mlruns/332362951122688275/traces/tr-ea1fca65e27a984d654821d07fcd9eb1/trace_info.yaml\n",
      "2026/01/11 23:34:10 WARNING mlflow.tracing.export.mlflow_v3: Failed to send trace to MLflow backend: Yaml file '/home/musel/Documents/github/TRACE/mlruns/332362951122688275/traces/tr-3f22faf823bed01d43cf2fde24933b83/trace_info.yaml' exists as '/home/musel/Documents/github/TRACE/mlruns/332362951122688275/traces/tr-3f22faf823bed01d43cf2fde24933b83/trace_info.yaml\n",
      "2026/01/11 23:34:11 WARNING mlflow.tracing.export.mlflow_v3: Failed to send trace to MLflow backend: Yaml file '/home/musel/Documents/github/TRACE/mlruns/332362951122688275/traces/tr-1745d6d87e570ddf827050a82369b584/trace_info.yaml' exists as '/home/musel/Documents/github/TRACE/mlruns/332362951122688275/traces/tr-1745d6d87e570ddf827050a82369b584/trace_info.yaml\n",
      "2026/01/11 23:34:11 WARNING mlflow.tracing.export.mlflow_v3: Failed to send trace to MLflow backend: Yaml file '/home/musel/Documents/github/TRACE/mlruns/332362951122688275/traces/tr-95a76d79bf3c4c06434308bc89fa6a68/trace_info.yaml' exists as '/home/musel/Documents/github/TRACE/mlruns/332362951122688275/traces/tr-95a76d79bf3c4c06434308bc89fa6a68/trace_info.yaml\n",
      "Evaluating: 100%|██████████| 10/10 [Elapsed: 00:00, Remaining: 00:00] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✨ Evaluation completed.\n",
      "\n",
      "Metrics and evaluation results are logged to the MLflow run:\n",
      "  Run name: \u001b[94mcatalog_embeddings|k=10|t=0.1|chat=gpt-4.1-mini|emb=text-embedding-3-small|f={}\u001b[0m\n",
      "  Run ID: \u001b[94mea0571e2c29f4862b1cfea04aa7516a9\u001b[0m\n",
      "\n",
      "To view the detailed evaluation results with sample-wise scores,\n",
      "open the \u001b[93m\u001b[1mTraces\u001b[0m tab in the Run page in the MLflow UI.\n",
      "\n",
      "DONE: catalog_embeddings|k=10|t=0.1|chat=gpt-4.1-mini|emb=text-embedding-3-small|f={}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 10/10 [Elapsed: 00:00, Remaining: 00:00] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✨ Evaluation completed.\n",
      "\n",
      "Metrics and evaluation results are logged to the MLflow run:\n",
      "  Run name: \u001b[94mfixed_window_embeddings|k=5|t=0.1|chat=gpt-4.1-mini|emb=text-embedding-3-small|f={}\u001b[0m\n",
      "  Run ID: \u001b[94m9281d1d9c1ea46efbf3c8eefaba91f08\u001b[0m\n",
      "\n",
      "To view the detailed evaluation results with sample-wise scores,\n",
      "open the \u001b[93m\u001b[1mTraces\u001b[0m tab in the Run page in the MLflow UI.\n",
      "\n",
      "DONE: fixed_window_embeddings|k=5|t=0.1|chat=gpt-4.1-mini|emb=text-embedding-3-small|f={}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 10/10 [Elapsed: 00:00, Remaining: 00:00] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✨ Evaluation completed.\n",
      "\n",
      "Metrics and evaluation results are logged to the MLflow run:\n",
      "  Run name: \u001b[94mfixed_window_embeddings|k=10|t=0.1|chat=gpt-4.1-mini|emb=text-embedding-3-small|f={}\u001b[0m\n",
      "  Run ID: \u001b[94m902c62ed69144bef8d01d4e7ae7d486d\u001b[0m\n",
      "\n",
      "To view the detailed evaluation results with sample-wise scores,\n",
      "open the \u001b[93m\u001b[1mTraces\u001b[0m tab in the Run page in the MLflow UI.\n",
      "\n",
      "DONE: fixed_window_embeddings|k=10|t=0.1|chat=gpt-4.1-mini|emb=text-embedding-3-small|f={}\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from openai import OpenAI\n",
    "\n",
    "mlflow.set_tracking_uri(\"file://\" + str((Path.cwd() / \"../mlruns\").resolve()))\n",
    "mlflow.set_experiment(\"xr_rag_variations_judged\")\n",
    "\n",
    "qdrant = QdrantClient(host=\"localhost\", port=6333)\n",
    "openai_client = OpenAI()\n",
    "\n",
    "with mlflow.start_run(run_name=\"parent_sweep\") as parent:\n",
    "    for cfg in cfgs:\n",
    "        run_name = f\"{cfg.vector_db}|k={cfg.top_k}|t={cfg.temperature}|chat={cfg.model_chat}|emb={cfg.model_embed}|f={cfg.filter_fields}\"\n",
    "        with mlflow.start_run(run_name=run_name, nested=True):\n",
    "            results = run_one_cfg_mlflow(\n",
    "                cfg=cfg,\n",
    "                dataset_rows=dataset,\n",
    "                qdrant=qdrant,\n",
    "                openai_client=openai_client,\n",
    "                max_examples=50,   # start small for iteration; remove later\n",
    "            )\n",
    "            print(\"DONE:\", run_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f47017d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from trl.experimental.judges import HfPairwiseJudge\n",
    "\n",
    "# judge = HfPairwiseJudge()\n",
    "# judge.judge(\n",
    "#     prompts=[\"What is the capital of France?\", \"What is the biggest planet in the solar system?\"],\n",
    "#     completions=[[\"Paris\", \"Lyon\"], [\"Saturn\", \"Jupiter\"]],\n",
    "# )  # Outputs: [0, 1]\n",
    "\n",
    "# mlflow ui --backend-store-uri file:./mlruns\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
