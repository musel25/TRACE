{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc3b9b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is set: True\n",
      "Length: 164\n",
      "Prefix: sk-proj...\n"
     ]
    }
   ],
   "source": [
    "# If needed:\n",
    "# pip install -U mlflow mlflow[genai] openai litellm qdrant-client pandas\n",
    "\n",
    "import os\n",
    "key = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(\"OPENAI_API_KEY is set:\", bool(key))\n",
    "print(\"Length:\", len(key) if key else None)\n",
    "print(\"Prefix:\", (key[:7] + \"...\") if key else None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a8b8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking: file:///home/musel/Documents/github/TRACE/notebooks/mlruns\n",
      "Experiment: xr_rag_llm_judge_sweep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/musel/Documents/github/TRACE/.venv/lib/python3.11/site-packages/mlflow/tracking/_tracking_service/utils.py:178: FutureWarning: The filesystem tracking backend (e.g., './mlruns') will be deprecated in February 2026. Consider transitioning to a database backend (e.g., 'sqlite:///mlflow.db') to take advantage of the latest MLflow features. See https://github.com/mlflow/mlflow/issues/18534 for more details and migration guidance. For migrating existing data, https://github.com/mlflow/mlflow-export-import can be used.\n",
      "  return FileStore(store_uri, store_uri)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import mlflow\n",
    "\n",
    "TRACKING_DIR = (Path.cwd() / \"mlruns\").resolve()\n",
    "mlflow.set_tracking_uri(\"file://\" + str(TRACKING_DIR))\n",
    "\n",
    "EXPERIMENT_NAME = \"xr_rag_llm_judge\"\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "print(\"Tracking:\", mlflow.get_tracking_uri())\n",
    "print(\"Experiment:\", EXPERIMENT_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ec5df20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.genai import make_judge\n",
    "\n",
    "JUDGE_NAME = \"xr_config_quality_score\"\n",
    "\n",
    "gt_judge = make_judge(\n",
    "    name=JUDGE_NAME,\n",
    "    instructions=(\n",
    "        \"You are grading an IOS XR telemetry configuration.\\n\\n\"\n",
    "        \"User request (inputs): {{ inputs }}\\n\\n\"\n",
    "        \"Candidate config (outputs): {{ outputs }}\\n\\n\"\n",
    "        \"Reference acceptable config (expectations): {{ expectations }}\\n\\n\"\n",
    "        \"Score quality from 0.0 to 1.0.\\n\"\n",
    "        \"Hard requirements (must match): IP, port, transport (grpc no-tls), encoding.\\n\"\n",
    "        \"Be lenient about names, ordering, and sample-interval unless requested.\\n\"\n",
    "        \"Penalize only if sensor-paths are clearly unrelated.\\n\"\n",
    "        \"Do NOT output telemetry config. Do NOT output code. Keep any explanation extremely short.\"\n",
    "    ),\n",
    "    feedback_value_type=float,\n",
    "    model=\"openai:/gpt-4.1-mini\",\n",
    "    inference_params={\"temperature\": 0, \"max_tokens\": 300},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a9081c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,\n",
       " RunCfg(vector_db='fixed_window_embeddings', top_k=5, filter_fields={}, temperature=0.0, model_chat='gpt-4.1-nano', model_embed='text-embedding-3-small'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class RunCfg:\n",
    "    vector_db: str\n",
    "    top_k: int\n",
    "    filter_fields: Dict[str, Any]\n",
    "    temperature: float\n",
    "    model_chat: str\n",
    "    model_embed: str\n",
    "\n",
    "def make_grid(\n",
    "    vector_dbs: List[str],\n",
    "    top_ks: List[int],\n",
    "    filters: List[Dict[str, Any]],\n",
    "    temps: List[float],\n",
    "    chat_models: List[str],\n",
    "    embed_models: List[str],\n",
    ") -> List[RunCfg]:\n",
    "    out: List[RunCfg] = []\n",
    "    for vdb in vector_dbs:\n",
    "        for k in top_ks:\n",
    "            for ff in filters:\n",
    "                for t in temps:\n",
    "                    for cm in chat_models:\n",
    "                        for em in embed_models:\n",
    "                            out.append(RunCfg(vdb, k, ff, t, cm, em))\n",
    "    return out\n",
    "\n",
    "VECTOR_DBS  = [\"fixed_window_embeddings\", \"catalog_embeddings_improved\"]\n",
    "TOP_KS      = [5]\n",
    "TEMPS       = [0.0]\n",
    "CHAT_MODELS = [\"gpt-4.1-nano\", \"gpt-4.1-mini\"]\n",
    "EMBED_MODELS = [\"text-embedding-3-small\"]\n",
    "FILTERS     = [{}]  # add your payload filters later if needed\n",
    "\n",
    "cfgs = make_grid(VECTOR_DBS, TOP_KS, FILTERS, TEMPS, CHAT_MODELS, EMBED_MODELS)\n",
    "len(cfgs), cfgs[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab69ba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import inspect\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd().resolve().parents[0]  # adjust if needed\n",
    "sys.path.insert(0, str(ROOT / \"src\"))\n",
    "\n",
    "from openai import OpenAI\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models as qmodels\n",
    "\n",
    "from tracerag.rag.naive import naive_rag, build_openai_chat_fn\n",
    "from tracerag.retrieval.qdrant import (\n",
    "    QdrantRetrievalConfig,\n",
    "    build_openai_embedding_fn,\n",
    "    build_qdrant_retriever,\n",
    ")\n",
    "\n",
    "def build_qdrant_filter(filter_fields: Dict[str, Any]) -> qmodels.Filter | None:\n",
    "    if not filter_fields:\n",
    "        return None\n",
    "    must = []\n",
    "    for k, v in filter_fields.items():\n",
    "        must.append(qmodels.FieldCondition(key=k, match=qmodels.MatchValue(value=v)))\n",
    "    return qmodels.Filter(must=must)\n",
    "\n",
    "def make_retriever(*, cfg: RunCfg, qdrant: QdrantClient, openai_client: OpenAI):\n",
    "    embed_fn = build_openai_embedding_fn(openai_client, model=cfg.model_embed)\n",
    "    q_filter = build_qdrant_filter(cfg.filter_fields)\n",
    "\n",
    "    config = QdrantRetrievalConfig(\n",
    "        collection_name=cfg.vector_db,\n",
    "        top_k=cfg.top_k,\n",
    "        query_filter=q_filter,\n",
    "    )\n",
    "\n",
    "    qdrant_retriever = build_qdrant_retriever(\n",
    "        qdrant=qdrant,\n",
    "        embedding_fn=embed_fn,\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    return lambda query, k: qdrant_retriever(query, top_k=k, query_filter=q_filter)\n",
    "\n",
    "def make_chat_fn(*, cfg: RunCfg, openai_client: OpenAI):\n",
    "    sig = inspect.signature(build_openai_chat_fn)\n",
    "    kwargs = {\"model\": cfg.model_chat, \"temperature\": cfg.temperature}\n",
    "\n",
    "    if \"openai_client\" in sig.parameters:\n",
    "        return build_openai_chat_fn(openai_client=openai_client, **kwargs)\n",
    "    if \"client\" in sig.parameters:\n",
    "        return build_openai_chat_fn(client=openai_client, **kwargs)\n",
    "\n",
    "    # last resort: positional client + kwargs\n",
    "    return build_openai_chat_fn(openai_client, **kwargs)\n",
    "\n",
    "def rag_predict_one(\n",
    "    *,\n",
    "    prompt: str,\n",
    "    cfg: RunCfg,\n",
    "    qdrant,\n",
    "    openai_client,\n",
    "    system_prompt: str,\n",
    ") -> str:\n",
    "    retriever = make_retriever(cfg=cfg, qdrant=qdrant, openai_client=openai_client)\n",
    "    chat_fn = make_chat_fn(cfg=cfg, openai_client=openai_client)\n",
    "\n",
    "    resp = naive_rag(\n",
    "        user_query=prompt,\n",
    "        retriever=retriever,\n",
    "        chat_fn=chat_fn,\n",
    "        system_prompt=system_prompt,\n",
    "        top_k=cfg.top_k,\n",
    "        answer_instruction=\"Return only IOS XR telemetry configuration.\",\n",
    "    )\n",
    "    return resp.answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a65af12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 100 rows\n",
      "Keys: dict_keys(['prompt', 'completion'])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "def load_jsonl(path: Path) -> List[Dict[str, Any]]:\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "DATASET_PATH = Path(\"../data/judge_dataset.jsonl\")  # <- change if needed\n",
    "dataset_rows = load_jsonl(DATASET_PATH)\n",
    "print(\"Loaded:\", len(dataset_rows), \"rows\")\n",
    "print(\"Keys:\", dataset_rows[0].keys())\n",
    "\n",
    "# Prompt variants: (name, system_prompt)\n",
    "BASE_SYSTEM_PROMPT = \"\"\n",
    "PROMPT_ENGINEERING = Path(\"../data/iosxr_prompt.txt\").read_text(encoding=\"utf-8\")\n",
    "PROMPT_VARIANTS = [\n",
    "    (\"base\", BASE_SYSTEM_PROMPT),\n",
    "    # Add more variants (different style, stricter rules, shorter, etc.)\n",
    "    (\"strict\", BASE_SYSTEM_PROMPT + PROMPT_ENGINEERING),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfc8418e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Dict, Any\n",
    "\n",
    "def build_eval_data(\n",
    "    dataset_rows: List[Dict[str, Any]],\n",
    "    cfg: RunCfg,\n",
    "    qdrant,\n",
    "    openai_client,\n",
    "    system_prompt: str,\n",
    "    max_examples: Optional[int] = None,\n",
    "):\n",
    "    n = len(dataset_rows) if max_examples is None else min(len(dataset_rows), max_examples)\n",
    "    eval_data = []\n",
    "\n",
    "    for i in range(n):\n",
    "        prompt = dataset_rows[i][\"prompt\"]\n",
    "        reference = dataset_rows[i][\"completion\"]\n",
    "\n",
    "        candidate = rag_predict_one(\n",
    "            prompt=prompt,\n",
    "            cfg=cfg,\n",
    "            qdrant=qdrant,\n",
    "            openai_client=openai_client,\n",
    "            system_prompt=system_prompt,\n",
    "        )\n",
    "\n",
    "        eval_data.append({\n",
    "            \"inputs\": {\"prompt\": prompt},\n",
    "            \"outputs\": candidate,  # keep it a plain string\n",
    "            \"expectations\": {\"expected_response\": reference},  # ✅ must be dict\n",
    "        })\n",
    "\n",
    "    return eval_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14fb1af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "def _maybe_json(x: Any) -> Any:\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip()\n",
    "        if (s.startswith(\"{\") and s.endswith(\"}\")) or (s.startswith(\"[\") and s.endswith(\"]\")):\n",
    "            try:\n",
    "                return json.loads(s)\n",
    "            except Exception:\n",
    "                return x\n",
    "    return x\n",
    "\n",
    "def traces_to_eval_df(traces: Any, judge_name: str) -> pd.DataFrame:\n",
    "    tdf = traces if isinstance(traces, pd.DataFrame) else pd.DataFrame(traces)\n",
    "    out_rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    for _, r in tdf.iterrows():\n",
    "        trace_id = r.get(\"trace_id\")\n",
    "\n",
    "        req = _maybe_json(r.get(\"request\"))\n",
    "        resp = _maybe_json(r.get(\"response\"))\n",
    "        exps = _maybe_json(r.get(\"expectations\"))  # <-- IMPORTANT\n",
    "        assessments = _maybe_json(r.get(\"assessments\"))\n",
    "\n",
    "        # ---- prompt ----\n",
    "        prompt = None\n",
    "        if isinstance(req, dict):\n",
    "            prompt = req.get(\"prompt\") or (req.get(\"inputs\") or {}).get(\"prompt\")\n",
    "\n",
    "        # ---- expected ----\n",
    "        expected = None\n",
    "        # 1) best: dedicated expectations column\n",
    "        if isinstance(exps, dict):\n",
    "            expected = exps.get(\"expected_response\") or exps.get(\"reference\") or exps.get(\"expected\")\n",
    "        elif isinstance(exps, str):\n",
    "            expected = exps\n",
    "\n",
    "        # 2) fallback: some MLflow variants tuck it into request\n",
    "        if expected is None and isinstance(req, dict):\n",
    "            exp_obj = req.get(\"expectations\")\n",
    "            if isinstance(exp_obj, dict):\n",
    "                expected = exp_obj.get(\"expected_response\") or exp_obj.get(\"reference\") or exp_obj.get(\"expected\")\n",
    "\n",
    "        # 3) fallback: sometimes flattened column exists\n",
    "        if expected is None:\n",
    "            expected = r.get(\"expected_response\")\n",
    "\n",
    "        # ---- candidate ----\n",
    "        candidate = None\n",
    "        if isinstance(resp, str):\n",
    "            candidate = resp\n",
    "        elif isinstance(resp, dict):\n",
    "            candidate = resp.get(\"outputs\") or resp.get(\"response\") or resp.get(\"output\") or resp.get(\"text\")\n",
    "            if isinstance(candidate, dict):\n",
    "                candidate = candidate.get(\"response\") or candidate.get(\"text\") or str(candidate)\n",
    "\n",
    "        # ---- judge score + rationale ----\n",
    "        score = None\n",
    "        rationale = None\n",
    "        if isinstance(assessments, list):\n",
    "            for a in assessments:\n",
    "                if not isinstance(a, dict):\n",
    "                    continue\n",
    "                name = a.get(\"assessment_name\") or a.get(\"name\")\n",
    "                if name == judge_name:\n",
    "                    fb = a.get(\"feedback\") or {}\n",
    "                    score = fb.get(\"value\")\n",
    "                    rationale = a.get(\"rationale\") or a.get(\"explanation\")\n",
    "                    break\n",
    "\n",
    "        out_rows.append({\n",
    "            \"trace_id\": trace_id,\n",
    "            \"prompt\": prompt,\n",
    "            \"expected\": expected,\n",
    "            \"candidate\": candidate,\n",
    "            \"score\": score,\n",
    "            \"rationale\": rationale,\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(out_rows, columns=[\"trace_id\",\"prompt\",\"expected\",\"candidate\",\"score\",\"rationale\"])\n",
    "    df[\"score\"] = pd.to_numeric(df[\"score\"], errors=\"coerce\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a45162cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import mlflow\n",
    "\n",
    "def run_one_cfg_mlflow(\n",
    "    *,\n",
    "    cfg,\n",
    "    dataset_rows,\n",
    "    qdrant,\n",
    "    openai_client,\n",
    "    system_prompt_name: str,\n",
    "    system_prompt: str,\n",
    "    max_examples=None,\n",
    "):\n",
    "    mlflow.log_params({\n",
    "        \"vector_db\": cfg.vector_db,\n",
    "        \"top_k\": cfg.top_k,\n",
    "        \"filter_fields\": json.dumps(cfg.filter_fields, sort_keys=True),\n",
    "        \"temperature\": cfg.temperature,\n",
    "        \"model_chat\": cfg.model_chat,\n",
    "        \"model_embed\": cfg.model_embed,\n",
    "        \"system_prompt_variant\": system_prompt_name,\n",
    "    })\n",
    "\n",
    "    eval_data = build_eval_data(\n",
    "        dataset_rows=dataset_rows,\n",
    "        cfg=cfg,\n",
    "        qdrant=qdrant,\n",
    "        openai_client=openai_client,\n",
    "        system_prompt=system_prompt,\n",
    "        max_examples=max_examples,\n",
    "    )\n",
    "\n",
    "    results = mlflow.genai.evaluate(data=eval_data, scorers=[gt_judge])\n",
    "\n",
    "    eval_run_id = results.run_id\n",
    "    mlflow.set_tag(\"eval_run_id\", eval_run_id)\n",
    "\n",
    "    traces = mlflow.search_traces(run_id=eval_run_id)\n",
    "    df = traces_to_eval_df(traces, judge_name=JUDGE_NAME)\n",
    "\n",
    "    # attach run context columns\n",
    "    df[\"system_prompt_variant\"] = system_prompt_name\n",
    "    df[\"vector_db\"] = cfg.vector_db\n",
    "    df[\"top_k\"] = cfg.top_k\n",
    "    df[\"temperature\"] = cfg.temperature\n",
    "    df[\"model_chat\"] = cfg.model_chat\n",
    "    df[\"model_embed\"] = cfg.model_embed\n",
    "\n",
    "    # --- aggregate metrics (defensive) ---\n",
    "    if (not df.empty) and (\"score\" in df.columns) and df[\"score\"].notna().any():\n",
    "        mlflow.log_metric(\"judge_mean\", float(df[\"score\"].mean()))\n",
    "        mlflow.log_metric(\"judge_min\", float(df[\"score\"].min()))\n",
    "        mlflow.log_metric(\"judge_pass_rate_ge_0.8\", float((df[\"score\"] >= 0.8).mean()))\n",
    "        mlflow.log_metric(\"n_examples\", int(df[\"score\"].notna().sum()))\n",
    "    else:\n",
    "        mlflow.log_metric(\"n_examples\", int(len(df)))\n",
    "        mlflow.log_metric(\"judge_mean\", 0.0)\n",
    "\n",
    "    # --- export artifact per run (Option A) ---\n",
    "    with tempfile.TemporaryDirectory() as td:\n",
    "        p = Path(td) / \"per_example_eval.csv\"\n",
    "        df.to_csv(p, index=False)\n",
    "        mlflow.log_artifact(str(p))\n",
    "        if hasattr(mlflow, \"log_table\"):\n",
    "            mlflow.log_table(df, \"per_example_eval_table.json\")\n",
    "\n",
    "    return df, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8aaadedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: small sample for quick test first\n",
    "import random\n",
    "random.seed(42)\n",
    "dataset_rows = random.sample(dataset_rows, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "325412ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/musel/Documents/github/TRACE/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2026/01/12 17:24:14 INFO mlflow.models.evaluation.utils.trace: Auto tracing is temporarily enabled during the model evaluation for computing some metrics and debugging. To disable tracing, call `mlflow.autolog(disable=True)`.\n",
      "Evaluating:   0%|          | 0/2 [Elapsed: 00:00, Remaining: ?] 2026/01/12 17:24:14 WARNING mlflow.tracing.export.mlflow_v3: Failed to send trace to MLflow backend: Yaml file '/home/musel/Documents/github/TRACE/notebooks/mlruns/356731495322076852/traces/tr-3eb13b9046685257bdd640fb06671ad1/trace_info.yaml' exists as '/home/musel/Documents/github/TRACE/notebooks/mlruns/356731495322076852/traces/tr-3eb13b9046685257bdd640fb06671ad1/trace_info.yaml\n",
      "2026/01/12 17:24:14 WARNING mlflow.tracing.export.mlflow_v3: Failed to send trace to MLflow backend: Yaml file '/home/musel/Documents/github/TRACE/notebooks/mlruns/356731495322076852/traces/tr-bd9c66b3ad3c2d6d1a3d1fa7bc8960a9/trace_info.yaml' exists as '/home/musel/Documents/github/TRACE/notebooks/mlruns/356731495322076852/traces/tr-bd9c66b3ad3c2d6d1a3d1fa7bc8960a9/trace_info.yaml\n",
      "/home/musel/Documents/github/TRACE/.venv/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='{\"result...: None}, annotations=[]), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "Evaluating: 100%|██████████| 2/2 [Elapsed: 00:05, Remaining: 00:00] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✨ Evaluation completed.\n",
      "\n",
      "Metrics and evaluation results are logged to the MLflow run:\n",
      "  Run name: \u001b[94mbase|fixed_window_embeddings|k=5|t=0.0|chat=gpt-4.1-nano|emb=text-embedding-3-small\u001b[0m\n",
      "  Run ID: \u001b[94m63199c3fbc504d10b7029dc551bfb8b8\u001b[0m\n",
      "\n",
      "To view the detailed evaluation results with sample-wise scores,\n",
      "open the \u001b[93m\u001b[1mTraces\u001b[0m tab in the Run page in the MLflow UI.\n",
      "\n",
      "DONE: base|fixed_window_embeddings|k=5|t=0.0|chat=gpt-4.1-nano|emb=text-embedding-3-small mean= nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/2 [Elapsed: 00:00, Remaining: ?] 2026/01/12 17:24:28 WARNING mlflow.tracing.export.mlflow_v3: Failed to send trace to MLflow backend: Yaml file '/home/musel/Documents/github/TRACE/notebooks/mlruns/356731495322076852/traces/tr-b38a088ca65ed389b74d0fb132e70629/trace_info.yaml' exists as '/home/musel/Documents/github/TRACE/notebooks/mlruns/356731495322076852/traces/tr-b38a088ca65ed389b74d0fb132e70629/trace_info.yaml\n",
      "2026/01/12 17:24:28 WARNING mlflow.tracing.export.mlflow_v3: Failed to send trace to MLflow backend: Yaml file '/home/musel/Documents/github/TRACE/notebooks/mlruns/356731495322076852/traces/tr-4737819096da1dac72ff5d2a386ecbe0/trace_info.yaml' exists as '/home/musel/Documents/github/TRACE/notebooks/mlruns/356731495322076852/traces/tr-4737819096da1dac72ff5d2a386ecbe0/trace_info.yaml\n",
      "Evaluating: 100%|██████████| 2/2 [Elapsed: 00:04, Remaining: 00:00] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✨ Evaluation completed.\n",
      "\n",
      "Metrics and evaluation results are logged to the MLflow run:\n",
      "  Run name: \u001b[94mbase|fixed_window_embeddings|k=5|t=0.0|chat=gpt-4.1-mini|emb=text-embedding-3-small\u001b[0m\n",
      "  Run ID: \u001b[94m69b4515bfd5a4bfbbcf141b297c80e8d\u001b[0m\n",
      "\n",
      "To view the detailed evaluation results with sample-wise scores,\n",
      "open the \u001b[93m\u001b[1mTraces\u001b[0m tab in the Run page in the MLflow UI.\n",
      "\n",
      "DONE: base|fixed_window_embeddings|k=5|t=0.0|chat=gpt-4.1-mini|emb=text-embedding-3-small mean= nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/2 [Elapsed: 00:00, Remaining: ?] 2026/01/12 17:24:36 WARNING mlflow.tracing.export.mlflow_v3: Failed to send trace to MLflow backend: Yaml file '/home/musel/Documents/github/TRACE/notebooks/mlruns/356731495322076852/traces/tr-1ff49b7889463e85759cde66bacfb3d0/trace_info.yaml' exists as '/home/musel/Documents/github/TRACE/notebooks/mlruns/356731495322076852/traces/tr-1ff49b7889463e85759cde66bacfb3d0/trace_info.yaml\n",
      "2026/01/12 17:24:36 WARNING mlflow.tracing.export.mlflow_v3: Failed to send trace to MLflow backend: Yaml file '/home/musel/Documents/github/TRACE/notebooks/mlruns/356731495322076852/traces/tr-4b0dbb418d5288f1142c3fe860e7a113/trace_info.yaml' exists as '/home/musel/Documents/github/TRACE/notebooks/mlruns/356731495322076852/traces/tr-4b0dbb418d5288f1142c3fe860e7a113/trace_info.yaml\n",
      "Evaluating: 100%|██████████| 2/2 [Elapsed: 00:02, Remaining: 00:00] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✨ Evaluation completed.\n",
      "\n",
      "Metrics and evaluation results are logged to the MLflow run:\n",
      "  Run name: \u001b[94mbase|catalog_embeddings_improved|k=5|t=0.0|chat=gpt-4.1-nano|emb=text-embedding-3-small\u001b[0m\n",
      "  Run ID: \u001b[94m435d6efb7cdf4090965c123817fa85f8\u001b[0m\n",
      "\n",
      "To view the detailed evaluation results with sample-wise scores,\n",
      "open the \u001b[93m\u001b[1mTraces\u001b[0m tab in the Run page in the MLflow UI.\n",
      "\n",
      "DONE: base|catalog_embeddings_improved|k=5|t=0.0|chat=gpt-4.1-nano|emb=text-embedding-3-small mean= nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/2 [Elapsed: 00:00, Remaining: ?] 2026/01/12 17:24:48 WARNING mlflow.tracing.export.mlflow_v3: Failed to send trace to MLflow backend: Yaml file '/home/musel/Documents/github/TRACE/notebooks/mlruns/356731495322076852/traces/tr-7412b29347294739614ff3d719db3ad0/trace_info.yaml' exists as '/home/musel/Documents/github/TRACE/notebooks/mlruns/356731495322076852/traces/tr-7412b29347294739614ff3d719db3ad0/trace_info.yaml\n",
      "2026/01/12 17:24:48 WARNING mlflow.tracing.export.mlflow_v3: Failed to send trace to MLflow backend: Yaml file '/home/musel/Documents/github/TRACE/notebooks/mlruns/356731495322076852/traces/tr-daf61a26146d3f31fc377a4c4a15544d/trace_info.yaml' exists as '/home/musel/Documents/github/TRACE/notebooks/mlruns/356731495322076852/traces/tr-daf61a26146d3f31fc377a4c4a15544d/trace_info.yaml\n",
      "Evaluating: 100%|██████████| 2/2 [Elapsed: 00:03, Remaining: 00:00] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✨ Evaluation completed.\n",
      "\n",
      "Metrics and evaluation results are logged to the MLflow run:\n",
      "  Run name: \u001b[94mbase|catalog_embeddings_improved|k=5|t=0.0|chat=gpt-4.1-mini|emb=text-embedding-3-small\u001b[0m\n",
      "  Run ID: \u001b[94m197dd2a4db1f48e6afad445f47ce55a1\u001b[0m\n",
      "\n",
      "To view the detailed evaluation results with sample-wise scores,\n",
      "open the \u001b[93m\u001b[1mTraces\u001b[0m tab in the Run page in the MLflow UI.\n",
      "\n",
      "DONE: base|catalog_embeddings_improved|k=5|t=0.0|chat=gpt-4.1-mini|emb=text-embedding-3-small mean= nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/2 [Elapsed: 00:00, Remaining: ?] 2026/01/12 17:24:56 WARNING mlflow.tracing.export.mlflow_v3: Failed to send trace to MLflow backend: Yaml file '/home/musel/Documents/github/TRACE/notebooks/mlruns/356731495322076852/traces/tr-88bd64072bcfbe01a28defe39bf00273/trace_info.yaml' exists as '/home/musel/Documents/github/TRACE/notebooks/mlruns/356731495322076852/traces/tr-88bd64072bcfbe01a28defe39bf00273/trace_info.yaml\n",
      "2026/01/12 17:24:56 WARNING mlflow.tracing.export.mlflow_v3: Failed to send trace to MLflow backend: Yaml file '/home/musel/Documents/github/TRACE/notebooks/mlruns/356731495322076852/traces/tr-451b4cf36123fdf77656af7229d4beef/trace_info.yaml' exists as '/home/musel/Documents/github/TRACE/notebooks/mlruns/356731495322076852/traces/tr-451b4cf36123fdf77656af7229d4beef/trace_info.yaml\n",
      "Evaluating: 100%|██████████| 2/2 [Elapsed: 00:03, Remaining: 00:00] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✨ Evaluation completed.\n",
      "\n",
      "Metrics and evaluation results are logged to the MLflow run:\n",
      "  Run name: \u001b[94mstrict|fixed_window_embeddings|k=5|t=0.0|chat=gpt-4.1-nano|emb=text-embedding-3-small\u001b[0m\n",
      "  Run ID: \u001b[94m6dfb9aa9bde24523965b569b80be93e7\u001b[0m\n",
      "\n",
      "To view the detailed evaluation results with sample-wise scores,\n",
      "open the \u001b[93m\u001b[1mTraces\u001b[0m tab in the Run page in the MLflow UI.\n",
      "\n",
      "DONE: strict|fixed_window_embeddings|k=5|t=0.0|chat=gpt-4.1-nano|emb=text-embedding-3-small mean= nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/2 [Elapsed: 00:00, Remaining: ?] 2026/01/12 17:25:05 WARNING mlflow.tracing.export.mlflow_v3: Failed to send trace to MLflow backend: Yaml file '/home/musel/Documents/github/TRACE/notebooks/mlruns/356731495322076852/traces/tr-66b2bc5b50c187fcce177b4e0837b8a3/trace_info.yaml' exists as '/home/musel/Documents/github/TRACE/notebooks/mlruns/356731495322076852/traces/tr-66b2bc5b50c187fcce177b4e0837b8a3/trace_info.yaml\n",
      "2026/01/12 17:25:05 WARNING mlflow.tracing.export.mlflow_v3: Failed to send trace to MLflow backend: Yaml file '/home/musel/Documents/github/TRACE/notebooks/mlruns/356731495322076852/traces/tr-9132b63ef16287e4e9c349e03602f8ac/trace_info.yaml' exists as '/home/musel/Documents/github/TRACE/notebooks/mlruns/356731495322076852/traces/tr-9132b63ef16287e4e9c349e03602f8ac/trace_info.yaml\n",
      "Evaluating: 100%|██████████| 2/2 [Elapsed: 00:06, Remaining: 00:00] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✨ Evaluation completed.\n",
      "\n",
      "Metrics and evaluation results are logged to the MLflow run:\n",
      "  Run name: \u001b[94mstrict|fixed_window_embeddings|k=5|t=0.0|chat=gpt-4.1-mini|emb=text-embedding-3-small\u001b[0m\n",
      "  Run ID: \u001b[94m2c74e28ef3ae4d2a8689fa691704d83a\u001b[0m\n",
      "\n",
      "To view the detailed evaluation results with sample-wise scores,\n",
      "open the \u001b[93m\u001b[1mTraces\u001b[0m tab in the Run page in the MLflow UI.\n",
      "\n",
      "DONE: strict|fixed_window_embeddings|k=5|t=0.0|chat=gpt-4.1-mini|emb=text-embedding-3-small mean= nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/2 [Elapsed: 00:00, Remaining: ?] 2026/01/12 17:25:16 WARNING mlflow.tracing.export.mlflow_v3: Failed to send trace to MLflow backend: Yaml file '/home/musel/Documents/github/TRACE/notebooks/mlruns/356731495322076852/traces/tr-89fa6a688fb5d27bbeb799193f22faf8/trace_info.yaml' exists as '/home/musel/Documents/github/TRACE/notebooks/mlruns/356731495322076852/traces/tr-89fa6a688fb5d27bbeb799193f22faf8/trace_info.yaml\n",
      "2026/01/12 17:25:16 WARNING mlflow.tracing.export.mlflow_v3: Failed to send trace to MLflow backend: Yaml file '/home/musel/Documents/github/TRACE/notebooks/mlruns/356731495322076852/traces/tr-956269f0e5d7b8756dadd6c795a76d79/trace_info.yaml' exists as '/home/musel/Documents/github/TRACE/notebooks/mlruns/356731495322076852/traces/tr-956269f0e5d7b8756dadd6c795a76d79/trace_info.yaml\n",
      "Evaluating: 100%|██████████| 2/2 [Elapsed: 00:05, Remaining: 00:00] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✨ Evaluation completed.\n",
      "\n",
      "Metrics and evaluation results are logged to the MLflow run:\n",
      "  Run name: \u001b[94mstrict|catalog_embeddings_improved|k=5|t=0.0|chat=gpt-4.1-nano|emb=text-embedding-3-small\u001b[0m\n",
      "  Run ID: \u001b[94m1ef3ff5615ab481fa383502b665bdd99\u001b[0m\n",
      "\n",
      "To view the detailed evaluation results with sample-wise scores,\n",
      "open the \u001b[93m\u001b[1mTraces\u001b[0m tab in the Run page in the MLflow UI.\n",
      "\n",
      "DONE: strict|catalog_embeddings_improved|k=5|t=0.0|chat=gpt-4.1-nano|emb=text-embedding-3-small mean= nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/2 [Elapsed: 00:00, Remaining: ?] 2026/01/12 17:25:27 WARNING mlflow.tracing.export.mlflow_v3: Failed to send trace to MLflow backend: Yaml file '/home/musel/Documents/github/TRACE/notebooks/mlruns/356731495322076852/traces/tr-ae340454cac5b68c28f49481a0a04dc4/trace_info.yaml' exists as '/home/musel/Documents/github/TRACE/notebooks/mlruns/356731495322076852/traces/tr-ae340454cac5b68c28f49481a0a04dc4/trace_info.yaml\n",
      "2026/01/12 17:25:27 WARNING mlflow.tracing.export.mlflow_v3: Failed to send trace to MLflow backend: Yaml file '/home/musel/Documents/github/TRACE/notebooks/mlruns/356731495322076852/traces/tr-988c24c961b1cd2262801c4510435a10/trace_info.yaml' exists as '/home/musel/Documents/github/TRACE/notebooks/mlruns/356731495322076852/traces/tr-988c24c961b1cd2262801c4510435a10/trace_info.yaml\n",
      "Evaluating: 100%|██████████| 2/2 [Elapsed: 00:03, Remaining: 00:00] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✨ Evaluation completed.\n",
      "\n",
      "Metrics and evaluation results are logged to the MLflow run:\n",
      "  Run name: \u001b[94mstrict|catalog_embeddings_improved|k=5|t=0.0|chat=gpt-4.1-mini|emb=text-embedding-3-small\u001b[0m\n",
      "  Run ID: \u001b[94m75c587858989407e9729f606cd4bc99c\u001b[0m\n",
      "\n",
      "To view the detailed evaluation results with sample-wise scores,\n",
      "open the \u001b[93m\u001b[1mTraces\u001b[0m tab in the Run page in the MLflow UI.\n",
      "\n",
      "DONE: strict|catalog_embeddings_improved|k=5|t=0.0|chat=gpt-4.1-mini|emb=text-embedding-3-small mean= nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Connect clients once\n",
    "qdrant = QdrantClient(host=\"localhost\", port=6333)\n",
    "openai_client = OpenAI()\n",
    "\n",
    "\n",
    "\n",
    "with mlflow.start_run(run_name=\"parent_sweep\") as parent:\n",
    "    for (pname, psys) in PROMPT_VARIANTS:\n",
    "        for cfg in cfgs:\n",
    "            run_name = f\"{pname}|{cfg.vector_db}|k={cfg.top_k}|t={cfg.temperature}|chat={cfg.model_chat}|emb={cfg.model_embed}\"\n",
    "            with mlflow.start_run(run_name=run_name, nested=True):\n",
    "                df, _ = run_one_cfg_mlflow(\n",
    "                    cfg=cfg,\n",
    "                    dataset_rows=dataset_rows,\n",
    "                    qdrant=qdrant,\n",
    "                    openai_client=openai_client,\n",
    "                    system_prompt_name=pname,\n",
    "                    system_prompt=psys,\n",
    "                    max_examples=50,  # bump/remove later\n",
    "                )\n",
    "                print(\"DONE:\", run_name, \"mean=\", df[\"score\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94ad812c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote per_example_all_runs_from_traces.csv rows: 28\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import mlflow\n",
    "\n",
    "exp = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "runs_df = mlflow.search_runs(experiment_ids=[exp.experiment_id], output_format=\"pandas\")\n",
    "child = runs_df[runs_df[\"tags.mlflow.runName\"] != \"parent_sweep\"].copy()\n",
    "\n",
    "all_dfs = []\n",
    "for _, rr in child.iterrows():\n",
    "    eval_run_id = rr.get(\"tags.eval_run_id\")\n",
    "    if pd.isna(eval_run_id) or eval_run_id is None:\n",
    "        # fallback if tag missing\n",
    "        eval_run_id = rr[\"run_id\"]\n",
    "\n",
    "    traces = mlflow.search_traces(run_id=str(eval_run_id))\n",
    "    df = traces_to_eval_df(traces, judge_name=JUDGE_NAME)\n",
    "\n",
    "    # attach run metadata you care about\n",
    "    df[\"run_id\"] = rr[\"run_id\"]\n",
    "    df[\"run_name\"] = rr.get(\"tags.mlflow.runName\")\n",
    "    df[\"vector_db\"] = rr.get(\"params.vector_db\")\n",
    "    df[\"top_k\"] = rr.get(\"params.top_k\")\n",
    "    df[\"temperature\"] = rr.get(\"params.temperature\")\n",
    "    df[\"model_chat\"] = rr.get(\"params.model_chat\")\n",
    "    df[\"system_prompt_variant\"] = rr.get(\"params.system_prompt_variant\")\n",
    "\n",
    "    all_dfs.append(df)\n",
    "\n",
    "per_example_all = pd.concat(all_dfs, ignore_index=True) if all_dfs else pd.DataFrame()\n",
    "per_example_all.to_csv(\"per_example_all_runs_from_traces.csv\", index=False)\n",
    "print(\"Wrote per_example_all_runs_from_traces.csv rows:\", len(per_example_all))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
